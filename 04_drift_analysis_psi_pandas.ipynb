{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Drift Analysis (Pandas Version)\n",
        "\n",
        "## Overview\n",
        "Comprehensive drift analysis using **pandas** with memory-efficient processing. This notebook performs drift detection for both **numerical features (PSI)** and **categorical features (Chi-Square)**, along with monthly trend analysis.\n",
        "\n",
        "### Analysis Types:\n",
        "1. **PSI Drift Analysis (Numerical Features)**: Population Stability Index comparing in-time vs OOT distributions\n",
        "2. **Chi-Square Drift Analysis (Categorical Features)**: Chi-square statistic comparing categorical distributions\n",
        "3. **Monthly Drift Trends**: Temporal evolution of drift for OOT months vs in-time baseline\n",
        "4. **Monthly Statistics Trends**: Median and average trends for numerical features across all months\n",
        "\n",
        "### Pandas Optimizations:\n",
        "- **Sampled full-table processing**: Load complete tables with sampling for accurate statistics\n",
        "- **Efficient binning**: Vectorized operations for PSI calculation\n",
        "- **Memory management**: Process one table at a time, free memory between tables\n",
        "- **Hybrid Spark/Pandas**: Use Spark for efficient Parquet reading, pandas for analysis\n",
        "\n",
        "### Drift Thresholds:\n",
        "**PSI (Numerical Features)**:\n",
        "- PSI < 0.1: Insignificant drift\n",
        "- 0.1 â‰¤ PSI < 0.25: Moderate drift (monitor)\n",
        "- PSI â‰¥ 0.25: Significant drift (investigate)\n",
        "\n",
        "**Chi-Square (Categorical Features)**:\n",
        "- Chi-square < 10.0: Insignificant drift\n",
        "- 10.0 â‰¤ Chi-square < 25.0: Moderate drift (monitor)\n",
        "- Chi-square â‰¥ 25.0: Significant drift (investigate)\n",
        "\n",
        "### Outputs:\n",
        "- **Numerical Features (PSI)**:\n",
        "  - PSI scores CSV (in-time vs OOT): `psi_overall_intime_vs_oot.csv`\n",
        "  - Monthly PSI CSVs per table: `psi_monthly_trends_{table_name}.csv`\n",
        "  - Individual monthly trend plots for each feature\n",
        "- **Categorical Features (Chi-Square)**:\n",
        "  - Chi-square scores CSV (in-time vs OOT): `chi_square_overall_intime_vs_oot.csv`\n",
        "  - Monthly chi-square CSVs per table: `chi_square_monthly_trends_{table_name}.csv`\n",
        "  - Individual monthly trend plots for each feature\n",
        "- **Monthly Statistics Trends**:\n",
        "  - Monthly median/average CSV per table: `monthly_statistics_trends_{table_name}.csv`\n",
        "  - Contains median and average for all numerical features across ALL months (in-time + OOT)\n",
        "- Overall drift visualizations: `plots/psi_distribution.png`, `plots/chi_square_distribution.png`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pandas==2 -i https://repo.td.com/repository/pypi-all/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import optimization libraries\n",
        "from joblib import Parallel, delayed\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optimized monthly trend plot creation (reusable for PSI and Chi-Square)\n",
        "def create_monthly_trend_plot(feature, feature_data, threshold_moderate, threshold_significant, \n",
        "                               table_folder_name, table_trend_folder, metric_name='PSI'):\n",
        "    \"\"\"Create monthly trend plot - optimized for parallel processing\"\"\"\n",
        "    try:\n",
        "        if len(feature_data) > 0:\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            metric_col = 'psi' if metric_name == 'PSI' else 'chi_square'\n",
        "            ax.plot(feature_data['month'], feature_data[metric_col], \n",
        "                   marker='o', linewidth=2, markersize=6, color='steelblue')\n",
        "            ax.axhline(y=threshold_moderate, color='orange', linestyle='--', linewidth=1.5, \n",
        "                      label=f'Moderate ({threshold_moderate})')\n",
        "            ax.axhline(y=threshold_significant, color='red', linestyle='--', linewidth=1.5, \n",
        "                      label=f'Significant ({threshold_significant})')\n",
        "            ax.set_title(f'Monthly {metric_name} Trend: {feature}\\n({table_folder_name})', \n",
        "                       fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(metric_name, fontsize=10)\n",
        "            ax.set_xlabel('Month', fontsize=10)\n",
        "            ax.legend(fontsize=9, loc='best')\n",
        "            ax.grid(True, alpha=0.3, linestyle='--')\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plot_file = f\"{table_trend_folder}{feature}.png\"\n",
        "            save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "            plt.close(fig)\n",
        "            return True\n",
        "        return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print(\"âœ“ Optimization functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import gc\n",
        "\n",
        "# Helper functions\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"âœ“ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    import tempfile, os\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    print(f\"âœ“ Saved plot to {adls_path}\")\n",
        "\n",
        "def calculate_psi(expected, actual, num_bins=10):\n",
        "    \"\"\"Calculate PSI between two distributions\"\"\"\n",
        "    try:\n",
        "        expected_clean = expected.dropna()\n",
        "        actual_clean = actual.dropna()\n",
        "        if len(expected_clean) == 0 or len(actual_clean) == 0:\n",
        "            return None\n",
        "        breakpoints = np.percentile(expected_clean, np.linspace(0, 100, num_bins + 1))\n",
        "        breakpoints = np.unique(breakpoints)\n",
        "        if len(breakpoints) <= 1:\n",
        "            return None\n",
        "        expected_binned = pd.cut(expected_clean, bins=breakpoints, include_lowest=True, duplicates='drop')\n",
        "        actual_binned = pd.cut(actual_clean, bins=breakpoints, include_lowest=True, duplicates='drop')\n",
        "        expected_pct = expected_binned.value_counts(normalize=True, sort=False) + 0.0001\n",
        "        actual_pct = actual_binned.value_counts(normalize=True, sort=False) + 0.0001\n",
        "        expected_pct, actual_pct = expected_pct.align(actual_pct, fill_value=0.0001)\n",
        "        psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
        "        return psi_value\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def calculate_chi_square(expected, actual):\n",
        "    \"\"\"Calculate Chi-square statistic between two categorical distributions\"\"\"\n",
        "    try:\n",
        "        from scipy.stats import chi2_contingency\n",
        "        \n",
        "        expected_clean = expected.dropna()\n",
        "        actual_clean = actual.dropna()\n",
        "        \n",
        "        if len(expected_clean) == 0 or len(actual_clean) == 0:\n",
        "            return None\n",
        "        \n",
        "        # Get all unique categories from both distributions\n",
        "        all_categories = set(expected_clean.unique()) | set(actual_clean.unique())\n",
        "        \n",
        "        if len(all_categories) <= 1:\n",
        "            return None  # No variation to measure\n",
        "        \n",
        "        # Create contingency table\n",
        "        expected_counts = expected_clean.value_counts()\n",
        "        actual_counts = actual_clean.value_counts()\n",
        "        \n",
        "        # Align to include all categories\n",
        "        expected_counts = expected_counts.reindex(all_categories, fill_value=0)\n",
        "        actual_counts = actual_counts.reindex(all_categories, fill_value=0)\n",
        "        \n",
        "        # Create 2xN contingency table\n",
        "        contingency = np.array([expected_counts.values, actual_counts.values])\n",
        "        \n",
        "        # Calculate chi-square statistic\n",
        "        chi2, p_value, dof, expected_freq = chi2_contingency(contingency)\n",
        "        \n",
        "        return chi2\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "print(\"âœ“ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/drift_analysis/\"\n",
        "PLOT_PATH = OUTPUT_PATH + \"plots/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "dbutils.fs.mkdirs(PLOT_PATH)\n",
        "\n",
        "SAMPLING_RATIO = 0.01\n",
        "OOT_START_DATE = '2024-01-01'\n",
        "PSI_THRESHOLD_MODERATE = 0.1\n",
        "PSI_THRESHOLD_SIGNIFICANT = 0.25\n",
        "CHI_SQUARE_THRESHOLD_MODERATE = 10.0  # Moderate drift threshold\n",
        "CHI_SQUARE_THRESHOLD_SIGNIFICANT = 25.0  # Significant drift threshold\n",
        "\n",
        "TABLES = [\n",
        "    (\"cust\", \"cust_basic_sumary\", ''),\n",
        "    (\"cust\", \"batch_credit_bureau\", ''),\n",
        "    (\"dem\", \"acct\", 2438),\n",
        "    (\"cc\", \"acct\", 2444),\n",
        "]\n",
        "\n",
        "# Load metadata\n",
        "feature_metadata_rows = spark.read.text(f\"{DATA_PATH}/feature/feature_metadata.jsonl\").collect()\n",
        "feature_metadata = json.loads('\\n'.join([row.value for row in feature_metadata_rows]))\n",
        "\n",
        "print(\"âœ“ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Strategy: Sampled Full-Table (Accuracy Prioritized)\n",
        "\n",
        "### Why This Approach?\n",
        "**PSI** (Population Stability Index) and **Chi-Square** both compare **complete distributions** between two time periods. These metrics **CANNOT be calculated incrementally** - we need to see all values to calculate accurate statistics.\n",
        "\n",
        "### Memory Efficiency:\n",
        "- **Memory usage**: Scales with SAMPLING_RATIO\n",
        "  - 1% sampling: ~2 GB per table\n",
        "  - 10% sampling: ~5 GB per table\n",
        "  - 100% sampling: ~15-20 GB per table\n",
        "- **Mitigation**: Process one table at a time (4-10 tables total), free memory between tables\n",
        "- **Recommendation**: Use `SAMPLING_RATIO = 0.01` (1%) for accurate results with manageable memory\n",
        "\n",
        "### How Drift Metrics Work:\n",
        "\n",
        "**PSI (Numerical Features)**:\n",
        "```\n",
        "PSI = Î£ (actual% - expected%) Ã— ln(actual% / expected%)\n",
        "\n",
        "Steps:\n",
        "  1. Load FULL table (sampled)\n",
        "  2. Split into in-time and OOT periods\n",
        "  3. For each numerical feature:\n",
        "     a. Create bins based on in-time distribution percentiles\n",
        "     b. Calculate % of values in each bin for in-time (expected)\n",
        "     c. Calculate % of values in each bin for OOT (actual)\n",
        "     d. Compute PSI across bins\n",
        "```\n",
        "\n",
        "**Chi-Square (Categorical Features)**:\n",
        "```\n",
        "Chi-Square = Î£ (observed - expected)Â² / expected\n",
        "\n",
        "Steps:\n",
        "  1. Load FULL table (sampled)\n",
        "  2. Split into in-time and OOT periods\n",
        "  3. For each categorical feature:\n",
        "     a. Get all unique categories from both periods\n",
        "     b. Create contingency table (2 rows Ã— N categories)\n",
        "     c. Calculate expected frequencies based on in-time distribution\n",
        "     d. Compute chi-square statistic\n",
        "```\n",
        "\n",
        "### Why This Requires Full Data:\n",
        "- âŒ **PSI**: Cannot bin incrementally - bins must span full value range\n",
        "- âŒ **PSI**: Cannot calculate percentages without seeing all values\n",
        "- âŒ **Chi-Square**: Need complete category frequency distributions\n",
        "- âŒ **Chi-Square**: Contingency table requires all categories from both periods\n",
        "- âœ… Must load all sampled values for both time periods\n",
        "- âœ… 1% sampling gives accurate drift metrics on representative sample\n",
        "\n",
        "### Drift Interpretation:\n",
        "\n",
        "**PSI (Numerical)**:\n",
        "- **PSI < 0.1**: Insignificant drift (no action needed)\n",
        "- **0.1 â‰¤ PSI < 0.25**: Moderate drift (monitor)\n",
        "- **PSI â‰¥ 0.25**: Significant drift (investigate feature)\n",
        "\n",
        "**Chi-Square (Categorical)**:\n",
        "- **Chi-square < 10.0**: Insignificant drift (no action needed)\n",
        "- **10.0 â‰¤ Chi-square < 25.0**: Moderate drift (monitor)\n",
        "- **Chi-square â‰¥ 25.0**: Significant drift (investigate feature)\n",
        "\n",
        "### Implementation:\n",
        "```\n",
        "For each table:\n",
        "  1. Load full table: spark.read.parquet().sample(0.01)\n",
        "  2. Convert to pandas: .toPandas()\n",
        "  3. Split: df_intime = df[df['efectv_dt'] < '2024-01-01']\n",
        "           df_oot = df[df['efectv_dt'] >= '2024-01-01']\n",
        "  4. For numerical features: Calculate PSI\n",
        "  5. For categorical features: Calculate Chi-Square\n",
        "  6. For monthly trends: Compare each OOT month vs in-time baseline\n",
        "  7. Free memory: del df, df_intime, df_oot; gc.collect()\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Criteria and Drift Assessment\n",
        "\n",
        "### âœ… **Analysis Succeeds If**:\n",
        "- **PSI calculated** for all numerical features\n",
        "- **Chi-Square calculated** for all categorical features\n",
        "- Drift levels classified (Insignificant/Moderate/Significant) for both types\n",
        "- Features with significant drift are flagged:\n",
        "  - Numerical: PSI â‰¥ 0.25\n",
        "  - Categorical: Chi-Square â‰¥ 25.0\n",
        "- Monthly drift trends show patterns (not random noise)\n",
        "- Monthly statistics trends (median/average) calculated for all numerical features\n",
        "- Results saved successfully to ADLS\n",
        "\n",
        "### ðŸ“Š **Drift Interpretation Guide**:\n",
        "\n",
        "**PSI (Numerical Features)**:\n",
        "| PSI Range | Drift Level | Action Required | Typical Cause |\n",
        "|-----------|-------------|-----------------|---------------|\n",
        "| < 0.1 | Insignificant | No action | Normal variation |\n",
        "| 0.1 - 0.25 | Moderate | Monitor | Slight distribution shift |\n",
        "| â‰¥ 0.25 | Significant | Investigate | Major distribution change |\n",
        "\n",
        "**Chi-Square (Categorical Features)**:\n",
        "| Chi-Square Range | Drift Level | Action Required | Typical Cause |\n",
        "|------------------|-------------|-----------------|---------------|\n",
        "| < 10.0 | Insignificant | No action | Normal variation |\n",
        "| 10.0 - 25.0 | Moderate | Monitor | Slight category distribution shift |\n",
        "| â‰¥ 25.0 | Significant | Investigate | Major category distribution change |\n",
        "\n",
        "### ðŸ” **Features to Flag**:\n",
        "After analysis, pay special attention to:\n",
        "- **PSI â‰¥ 0.25**: Significant numerical drift - may impact model performance\n",
        "- **Chi-Square â‰¥ 25.0**: Significant categorical drift - category distributions changed\n",
        "- **Increasing drift trends**: Growing drift over OOT months (both metrics)\n",
        "- **Sudden drift spikes**: Abrupt changes in specific months\n",
        "- **Bureau features**: Often have higher drift due to external factors\n",
        "- **High-cardinality categoricals**: May show higher chi-square due to sparse categories\n",
        "\n",
        "### ðŸ“ˆ **Expected Patterns**:\n",
        "- **Most numerical features**: PSI < 0.1 (stable distributions)\n",
        "- **Most categorical features**: Chi-Square < 10.0 (stable category frequencies)\n",
        "- **Some features**: Moderate drift (0.1 â‰¤ PSI < 0.25 or 10 â‰¤ Chi-Square < 25) - acceptable\n",
        "- **Few features**: Significant drift (PSI â‰¥ 0.25 or Chi-Square â‰¥ 25.0) - investigate\n",
        "- **Bureau features**: May have higher drift (external data source)\n",
        "- **Transaction features**: Usually stable drift (internal data)\n",
        "- **Monthly statistics**: Should show gradual trends, not abrupt jumps\n",
        "\n",
        "### ðŸ“Š **Monthly Statistics Trends**:\n",
        "- **Median trends**: Should be relatively stable across months\n",
        "- **Average trends**: May fluctuate more than median (sensitive to outliers)\n",
        "- **In-Time vs OOT**: Compare trends before/after 2024 boundary\n",
        "- **Gradual changes**: Expected (business cycles, seasonality)\n",
        "- **Abrupt changes**: Investigate (data quality issues, feature engineering changes)\n",
        "\n",
        "### âš ï¸ **When to Investigate**:\n",
        "- More than 20% of features have significant drift\n",
        "- Core features (demographics, balances) show high drift\n",
        "- Drift increases month-over-month in OOT period\n",
        "- Monthly statistics show abrupt jumps (not gradual trends)\n",
        "- Drift patterns differ significantly by table/family\n",
        "- Categorical features with very high chi-square (possible data issues)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PSI DRIFT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_psi_results = []\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        continue\n",
        "    \n",
        "    num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "    \n",
        "    # Load via spark then convert to pandas\n",
        "    df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "    if 'efectv_dt' not in df_spark.columns:\n",
        "        continue\n",
        "    \n",
        "    if SAMPLING_RATIO < 1.0:\n",
        "        df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    \n",
        "    df = df_spark.select(['efectv_dt'] + [c for c in num_features if c in df_spark.columns]).toPandas()\n",
        "    df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "    \n",
        "    # Split data\n",
        "    # Convert OOT_START_DATE to datetime for comparison\n",
        "    oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "\n",
        "    df_intime = df[df['efectv_dt'] < oot_date]\n",
        "    df_oot = df[df['efectv_dt'] >= oot_date]\n",
        "    \n",
        "    # Calculate PSI for each feature\n",
        "    for feature in num_features:\n",
        "        if feature not in df.columns:\n",
        "            continue\n",
        "        psi = calculate_psi(df_intime[feature], df_oot[feature])\n",
        "        if psi is not None:\n",
        "            drift_level = 'Significant' if psi >= PSI_THRESHOLD_SIGNIFICANT else \\\n",
        "                         'Moderate' if psi >= PSI_THRESHOLD_MODERATE else 'Insignificant'\n",
        "            all_psi_results.append({\n",
        "                'table': f\"{fam_name}_{table}\",\n",
        "                'feature': feature,\n",
        "                'psi': psi,\n",
        "                'drift_level': drift_level\n",
        "            })\n",
        "    \n",
        "    del df, df_spark, df_intime, df_oot\n",
        "    gc.collect()\n",
        "\n",
        "# Save results and create visualizations\n",
        "if all_psi_results:\n",
        "    psi_df = pd.DataFrame(all_psi_results).sort_values('psi', ascending=False)\n",
        "    save_pandas_to_csv_adls(psi_df, OUTPUT_PATH + \"psi_overall_intime_vs_oot.csv\")\n",
        "    \n",
        "    # Create drift analysis summary\n",
        "    summary = {\n",
        "        'analysis_date': pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        'sampling_ratio': SAMPLING_RATIO,\n",
        "        'total_features': len(psi_df),\n",
        "        'insignificant_drift': len(psi_df[psi_df['drift_level'] == 'Insignificant']),\n",
        "        'moderate_drift': len(psi_df[psi_df['drift_level'] == 'Moderate']),\n",
        "        'significant_drift': len(psi_df[psi_df['drift_level'] == 'Significant']),\n",
        "        'mean_psi': float(psi_df['psi'].mean()),\n",
        "        'median_psi': float(psi_df['psi'].median()),\n",
        "        'max_psi': float(psi_df['psi'].max()),\n",
        "        'min_psi': float(psi_df['psi'].min()),\n",
        "    }\n",
        "    summary_df = pd.DataFrame([summary])\n",
        "    save_pandas_to_csv_adls(summary_df, OUTPUT_PATH + \"drift_analysis_summary.csv\")\n",
        "    \n",
        "    # PSI distribution visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    axes[0].hist(psi_df['psi'], bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(x=PSI_THRESHOLD_MODERATE, color='orange', linestyle='--', linewidth=2, label='Moderate')\n",
        "    axes[0].axvline(x=PSI_THRESHOLD_SIGNIFICANT, color='red', linestyle='--', linewidth=2, label='Significant')\n",
        "    axes[0].set_xlabel('PSI Value', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[0].set_title('PSI Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    \n",
        "    drift_counts = psi_df['drift_level'].value_counts()\n",
        "    colors = {'Insignificant': 'green', 'Moderate': 'orange', 'Significant': 'red'}\n",
        "    bar_colors = [colors.get(x, 'gray') for x in drift_counts.index]\n",
        "    axes[1].bar(drift_counts.index, drift_counts.values, color=bar_colors, edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_xlabel('Drift Level', fontsize=12)\n",
        "    axes[1].set_ylabel('Count', fontsize=12)\n",
        "    axes[1].set_title('Features by Drift Level', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot_to_adls(fig, PLOT_PATH + \"psi_distribution.png\", dpi=150)\n",
        "    plt.close(fig)\n",
        "    \n",
        "    print(f\"\\nâœ“ Analyzed {len(psi_df)} features\")\n",
        "    print(f\"  Insignificant drift: {summary['insignificant_drift']}\")\n",
        "    print(f\"  Moderate drift: {summary['moderate_drift']}\")\n",
        "    print(f\"  Significant drift: {summary['significant_drift']}\")\n",
        "    print(f\"  Mean PSI: {summary['mean_psi']:.4f}\")\n",
        "\n",
        "print(\"\\nâœ“ Drift analysis complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CHI-SQUARE DRIFT ANALYSIS (CATEGORICAL FEATURES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_chi_square_results = []\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        continue\n",
        "    \n",
        "    cat_features = list(feature_metadata[fam_name][table_meta_key].get(\"cat_features\", {}).keys())\n",
        "    \n",
        "    if len(cat_features) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Load via spark then convert to pandas\n",
        "    df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "    if 'efectv_dt' not in df_spark.columns:\n",
        "        continue\n",
        "    \n",
        "    if SAMPLING_RATIO < 1.0:\n",
        "        df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    \n",
        "    df = df_spark.select(['efectv_dt'] + [c for c in cat_features if c in df_spark.columns]).toPandas()\n",
        "    df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "    \n",
        "    # Convert OOT_START_DATE to datetime for comparison\n",
        "    oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "    \n",
        "    # Split data\n",
        "    df_intime = df[df['efectv_dt'] < oot_date]\n",
        "    df_oot = df[df['efectv_dt'] >= oot_date]\n",
        "    \n",
        "    # Calculate Chi-square for each categorical feature\n",
        "    for feature in cat_features:\n",
        "        if feature not in df.columns:\n",
        "            continue\n",
        "        chi2 = calculate_chi_square(df_intime[feature], df_oot[feature])\n",
        "        if chi2 is not None:\n",
        "            drift_level = 'Significant' if chi2 >= CHI_SQUARE_THRESHOLD_SIGNIFICANT else \\\n",
        "                         'Moderate' if chi2 >= CHI_SQUARE_THRESHOLD_MODERATE else 'Insignificant'\n",
        "            all_chi_square_results.append({\n",
        "                'table': f\"{fam_name}_{table}\",\n",
        "                'feature': feature,\n",
        "                'chi_square': chi2,\n",
        "                'drift_level': drift_level\n",
        "            })\n",
        "    \n",
        "    del df, df_spark, df_intime, df_oot\n",
        "    gc.collect()\n",
        "\n",
        "# Save results and create visualizations\n",
        "if all_chi_square_results:\n",
        "    chi2_df = pd.DataFrame(all_chi_square_results).sort_values('chi_square', ascending=False)\n",
        "    save_pandas_to_csv_adls(chi2_df, OUTPUT_PATH + \"chi_square_overall_intime_vs_oot.csv\")\n",
        "    \n",
        "    # Chi-square distribution visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    axes[0].hist(chi2_df['chi_square'], bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(x=CHI_SQUARE_THRESHOLD_MODERATE, color='orange', linestyle='--', linewidth=2, label='Moderate')\n",
        "    axes[0].axvline(x=CHI_SQUARE_THRESHOLD_SIGNIFICANT, color='red', linestyle='--', linewidth=2, label='Significant')\n",
        "    axes[0].set_xlabel('Chi-Square Value', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[0].set_title('Chi-Square Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    \n",
        "    drift_counts = chi2_df['drift_level'].value_counts()\n",
        "    colors = {'Insignificant': 'green', 'Moderate': 'orange', 'Significant': 'red'}\n",
        "    bar_colors = [colors.get(x, 'gray') for x in drift_counts.index]\n",
        "    axes[1].bar(drift_counts.index, drift_counts.values, color=bar_colors, edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_xlabel('Drift Level', fontsize=12)\n",
        "    axes[1].set_ylabel('Count', fontsize=12)\n",
        "    axes[1].set_title('Features by Drift Level', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot_to_adls(fig, PLOT_PATH + \"chi_square_distribution.png\", dpi=150)\n",
        "    plt.close(fig)\n",
        "    \n",
        "    print(f\"\\nâœ“ Analyzed {len(chi2_df)} categorical features\")\n",
        "    print(f\"  Insignificant drift: {len(chi2_df[chi2_df['drift_level'] == 'Insignificant'])}\")\n",
        "    print(f\"  Moderate drift: {len(chi2_df[chi2_df['drift_level'] == 'Moderate'])}\")\n",
        "    print(f\"  Significant drift: {len(chi2_df[chi2_df['drift_level'] == 'Significant'])}\")\n",
        "    print(f\"  Mean Chi-square: {chi2_df['chi_square'].mean():.4f}\")\n",
        "\n",
        "print(\"\\nâœ“ Chi-square drift analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monthly PSI Trends (Numerical Features)\n",
        "\n",
        "Calculate PSI for each OOT month to show temporal evolution of drift for all numerical features.\n",
        "\n",
        "### Output Structure:\n",
        "- Individual trend plots for each feature (one PNG per feature)\n",
        "- Saved in table-specific folders: `plots/monthly_trends/{table_name}/`\n",
        "- Each file named: `{feature_name}.png`\n",
        "- Monthly PSI CSV per table: `psi_monthly_trends_{table_name}.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monthly Chi-Square Trends (Categorical Features)\n",
        "\n",
        "Calculate Chi-square for each OOT month to show temporal evolution of drift for all categorical features.\n",
        "\n",
        "### Output Structure:\n",
        "- Individual trend plots for each feature (one PNG per feature)\n",
        "- Saved in table-specific folders: `plots/monthly_trends_chi_square/{table_name}/`\n",
        "- Each file named: `{feature_name}.png`\n",
        "- Monthly Chi-square CSV per table: `chi_square_monthly_trends_{table_name}.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly Chi-Square trends for all tables (categorical features)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MONTHLY CHI-SQUARE TREND ANALYSIS (CATEGORICAL FEATURES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MONTHLY_TRENDS_CHI_PLOT_PATH = PLOT_PATH + \"monthly_trends_chi_square/\"\n",
        "dbutils.fs.mkdirs(MONTHLY_TRENDS_CHI_PLOT_PATH)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing monthly chi-square trends: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        print(f\"  Skipping (not in metadata)\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Load table\n",
        "        df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "        \n",
        "        if 'efectv_dt' not in df_spark.columns:\n",
        "            print(f\"  Skipping (no efectv_dt column)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        if SAMPLING_RATIO < 1.0:\n",
        "            df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "        \n",
        "        # Get ALL categorical features\n",
        "        cat_features = list(feature_metadata[fam_name][table_meta_key].get(\"cat_features\", {}).keys())\n",
        "        cat_features = [f for f in cat_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(cat_features) == 0:\n",
        "            print(f\"  Skipping (no categorical features)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        df = df_spark.select(['efectv_dt'] + cat_features).toPandas()\n",
        "        df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "        \n",
        "        # Convert OOT_START_DATE to datetime for comparison\n",
        "        oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "        \n",
        "        # Separate in-time and OOT\n",
        "        df_intime = df[df['efectv_dt'] < oot_date]\n",
        "        df_oot = df[df['efectv_dt'] >= oot_date]\n",
        "        \n",
        "        if len(df_intime) == 0 or len(df_oot) == 0:\n",
        "            print(f\"  Skipping (insufficient data)\")\n",
        "            del df, df_intime, df_oot, df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        # Get unique OOT months\n",
        "        oot_months = df_oot['efectv_dt'].dt.to_period('M').unique()\n",
        "        \n",
        "        if len(oot_months) == 0:\n",
        "            print(f\"  Skipping (no OOT months)\")\n",
        "            del df, df_intime, df_oot, df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        # Create folder for this table's monthly trend plots\n",
        "        table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "        table_trend_folder = f\"{MONTHLY_TRENDS_CHI_PLOT_PATH}{table_folder_name}/\"\n",
        "        dbutils.fs.mkdirs(table_trend_folder)\n",
        "        \n",
        "        # Calculate Chi-square for each month and feature\n",
        "        monthly_chi2_results = []\n",
        "        print(f\"  Calculating Chi-square for {len(cat_features)} features across {len(oot_months)} months...\")\n",
        "        \n",
        "        for month in sorted(oot_months):\n",
        "            df_month = df_oot[df_oot['efectv_dt'].dt.to_period('M') == month]\n",
        "            for feature in cat_features:\n",
        "                chi2 = calculate_chi_square(df_intime[feature], df_month[feature])\n",
        "                if chi2 is not None:\n",
        "                    monthly_chi2_results.append({\n",
        "                        'month': str(month),\n",
        "                        'feature': feature,\n",
        "                        'chi_square': chi2\n",
        "                    })\n",
        "        \n",
        "        # Save monthly Chi-square results CSV per table\n",
        "        if monthly_chi2_results:\n",
        "            monthly_chi2_df = pd.DataFrame(monthly_chi2_results)\n",
        "            csv_file = f\"{OUTPUT_PATH}chi_square_monthly_trends_{table_folder_name}.csv\"\n",
        "            save_pandas_to_csv_adls(monthly_chi2_df, csv_file)\n",
        "            \n",
        "            # Create individual trend plots for each feature\n",
        "            print(f\"  Creating {len(cat_features)} individual trend plots...\")\n",
        "            saved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            for feature in cat_features:\n",
        "                fig = None\n",
        "                try:\n",
        "                    # Get data for this feature\n",
        "                    feature_data = monthly_chi2_df[monthly_chi2_df['feature'] == feature].sort_values('month')\n",
        "                    \n",
        "                    if len(feature_data) > 0:\n",
        "                        # Create individual figure for each feature\n",
        "                        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "                        \n",
        "                        # Plot trend\n",
        "                        ax.plot(feature_data['month'], feature_data['chi_square'], \n",
        "                               marker='o', linewidth=2, markersize=6, color='steelblue')\n",
        "                        \n",
        "                        # Add threshold lines\n",
        "                        ax.axhline(y=CHI_SQUARE_THRESHOLD_MODERATE, color='orange', \n",
        "                                  linestyle='--', linewidth=1.5, label='Moderate (10.0)')\n",
        "                        ax.axhline(y=CHI_SQUARE_THRESHOLD_SIGNIFICANT, color='red', \n",
        "                                  linestyle='--', linewidth=1.5, label='Significant (25.0)')\n",
        "                        \n",
        "                        # Style the plot\n",
        "                        ax.set_title(f'Monthly Chi-Square Trend: {feature}\\n({table_folder_name})', \n",
        "                                   fontsize=12, fontweight='bold')\n",
        "                        ax.set_ylabel('Chi-Square', fontsize=10)\n",
        "                        ax.set_xlabel('Month', fontsize=10)\n",
        "                        ax.legend(fontsize=9, loc='best')\n",
        "                        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "                        ax.tick_params(axis='x', rotation=45)\n",
        "                        \n",
        "                        plt.tight_layout()\n",
        "                        \n",
        "                        # Save individual plot (no display - saved directly to ADLS)\n",
        "                        plot_file = f\"{table_trend_folder}{feature}.png\"\n",
        "                        save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "                        plt.close(fig)  # Explicitly close to free memory\n",
        "                        fig = None  # Prevent double-close\n",
        "                        saved_count += 1\n",
        "                    else:\n",
        "                        if fig is not None:\n",
        "                            plt.close(fig)\n",
        "                        failed_count += 1\n",
        "                        print(f\"    Warning: Skipped {feature} (no data)\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    if fig is not None:\n",
        "                        plt.close(fig)\n",
        "                    failed_count += 1\n",
        "                    print(f\"    Warning: Could not plot {feature}: {str(e)}\")\n",
        "            \n",
        "            print(f\"  âœ“ Monthly trend plots saved: {saved_count} successful, {failed_count} failed\")\n",
        "            print(f\"    Location: {table_trend_folder}\")\n",
        "        \n",
        "        del df, df_intime, df_oot, df_spark\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing {fam_name}-{table}: {str(e)}\")\n",
        "        try:\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"\\nâœ“ Monthly Chi-Square trend analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monthly Statistics Trends (Median & Average for Numerical Features)\n",
        "\n",
        "Calculate monthly median and average for ALL numerical features across ALL months (both in-time and OOT).\n",
        "\n",
        "### Output Format:\n",
        "- CSV per table: `monthly_statistics_trends_{table_name}.csv`\n",
        "- Columns: `feature_name`, `stat_method`, `month1`, `month2`, `month3`, ...\n",
        "- Rows: Each feature has two rows (median and average)\n",
        "- Format example:\n",
        "  - `feature1, median, val_month1, val_month2, ...`\n",
        "  - `feature1, average, val_month1, val_month2, ...`\n",
        "  - `feature2, median, val_month1, val_month2, ...`\n",
        "  - `feature2, average, val_month1, val_month2, ...`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly statistics trends (median and average) for all numerical features across ALL months\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MONTHLY STATISTICS TRENDS (MEDIAN & AVERAGE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing monthly statistics: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        print(f\"  Skipping (not in metadata)\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Load table\n",
        "        df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "        \n",
        "        if 'efectv_dt' not in df_spark.columns:\n",
        "            print(f\"  Skipping (no efectv_dt column)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        if SAMPLING_RATIO < 1.0:\n",
        "            df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "        \n",
        "        # Get ALL numerical features\n",
        "        num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "        num_features = [f for f in num_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(num_features) == 0:\n",
        "            print(f\"  Skipping (no numerical features)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        df = df_spark.select(['efectv_dt'] + num_features).toPandas()\n",
        "        df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "        \n",
        "        # Convert OOT_START_DATE to datetime for comparison (if needed later)\n",
        "        oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "        \n",
        "        # Get ALL unique months (both in-time and OOT)\n",
        "        df['month'] = df['efectv_dt'].dt.to_period('M')\n",
        "        all_months = sorted(df['month'].unique())\n",
        "        \n",
        "        if len(all_months) == 0:\n",
        "            print(f\"  Skipping (no months found)\")\n",
        "            del df, df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        print(f\"  Calculating statistics for {len(num_features)} features across {len(all_months)} months...\")\n",
        "        \n",
        "        # Build statistics table\n",
        "        stats_rows = []\n",
        "        \n",
        "        for feature in num_features:\n",
        "            if feature not in df.columns:\n",
        "                continue\n",
        "            \n",
        "            # Calculate median and average for each month\n",
        "            median_values = {}\n",
        "            average_values = {}\n",
        "            \n",
        "            for month in all_months:\n",
        "                month_data = df[df['month'] == month][feature].dropna()\n",
        "                if len(month_data) > 0:\n",
        "                    median_values[str(month)] = month_data.median()\n",
        "                    average_values[str(month)] = month_data.mean()\n",
        "                else:\n",
        "                    median_values[str(month)] = None\n",
        "                    average_values[str(month)] = None\n",
        "            \n",
        "            # Create row for median\n",
        "            median_row = {'feature_name': feature, 'stat_method': 'median'}\n",
        "            median_row.update({str(m): median_values[str(m)] for m in all_months})\n",
        "            stats_rows.append(median_row)\n",
        "            \n",
        "            # Create row for average\n",
        "            average_row = {'feature_name': feature, 'stat_method': 'average'}\n",
        "            average_row.update({str(m): average_values[str(m)] for m in all_months})\n",
        "            stats_rows.append(average_row)\n",
        "        \n",
        "        # Convert to DataFrame and save\n",
        "        if stats_rows:\n",
        "            stats_df = pd.DataFrame(stats_rows)\n",
        "            # Reorder columns: feature_name, stat_method, then months in order\n",
        "            col_order = ['feature_name', 'stat_method'] + [str(m) for m in all_months]\n",
        "            stats_df = stats_df[col_order]\n",
        "            \n",
        "            table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "            csv_file = f\"{OUTPUT_PATH}monthly_statistics_trends_{table_folder_name}.csv\"\n",
        "            save_pandas_to_csv_adls(stats_df, csv_file)\n",
        "            print(f\"  âœ“ Saved monthly statistics trends for {len(num_features)} features\")\n",
        "        \n",
        "        del df, df_spark\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing {fam_name}-{table}: {str(e)}\")\n",
        "        try:\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"\\nâœ“ Monthly statistics trends analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chi-Square Overall Analysis (In-Time vs OOT)\n",
        "\n",
        "Calculate Chi-square statistic between in-time and OOT distributions for categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly PSI trends for all tables\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MONTHLY PSI TREND ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MONTHLY_TRENDS_PLOT_PATH = PLOT_PATH + \"monthly_trends/\"\n",
        "dbutils.fs.mkdirs(MONTHLY_TRENDS_PLOT_PATH)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing monthly trends: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        print(f\"  Skipping (not in metadata)\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Load table\n",
        "        df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "        \n",
        "        if 'efectv_dt' not in df_spark.columns:\n",
        "            print(f\"  Skipping (no efectv_dt column)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        if SAMPLING_RATIO < 1.0:\n",
        "            df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "        \n",
        "        # Get ALL numerical features (not just first 10)\n",
        "        num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "        num_features = [f for f in num_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(num_features) == 0:\n",
        "            print(f\"  Skipping (no numerical features)\")\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        df = df_spark.select(['efectv_dt'] + num_features).toPandas()\n",
        "        df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "        \n",
        "        # Convert OOT_START_DATE to datetime for comparison\n",
        "        oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "        \n",
        "        # Separate in-time and OOT\n",
        "        df_intime = df[df['efectv_dt'] < oot_date]\n",
        "        df_oot = df[df['efectv_dt'] >= oot_date]\n",
        "        \n",
        "        if len(df_intime) == 0 or len(df_oot) == 0:\n",
        "            print(f\"  Skipping (insufficient data)\")\n",
        "            del df, df_intime, df_oot, df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        # Get unique OOT months\n",
        "        oot_months = df_oot['efectv_dt'].dt.to_period('M').unique()\n",
        "        \n",
        "        if len(oot_months) == 0:\n",
        "            print(f\"  Skipping (no OOT months)\")\n",
        "            del df, df_intime, df_oot, df_spark\n",
        "            gc.collect()\n",
        "            continue\n",
        "        \n",
        "        # Create folder for this table's monthly trend plots\n",
        "        table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "        table_trend_folder = f\"{MONTHLY_TRENDS_PLOT_PATH}{table_folder_name}/\"\n",
        "        dbutils.fs.mkdirs(table_trend_folder)\n",
        "        \n",
        "        # Calculate PSI for each month and feature\n",
        "        monthly_psi_results = []\n",
        "        print(f\"  Calculating PSI for {len(num_features)} features across {len(oot_months)} months...\")\n",
        "        \n",
        "        for month in sorted(oot_months):\n",
        "            df_month = df_oot[df_oot['efectv_dt'].dt.to_period('M') == month]\n",
        "            for feature in num_features:\n",
        "                psi = calculate_psi(df_intime[feature], df_month[feature])\n",
        "                if psi is not None:\n",
        "                    monthly_psi_results.append({\n",
        "                        'month': str(month),\n",
        "                        'feature': feature,\n",
        "                        'psi': psi\n",
        "                    })\n",
        "        \n",
        "        # Save monthly PSI results CSV per table\n",
        "        if monthly_psi_results:\n",
        "            monthly_psi_df = pd.DataFrame(monthly_psi_results)\n",
        "            csv_file = f\"{OUTPUT_PATH}psi_monthly_trends_{table_folder_name}.csv\"\n",
        "            save_pandas_to_csv_adls(monthly_psi_df, csv_file)\n",
        "            \n",
        "            # Create individual trend plots for each feature\n",
        "            print(f\"  Creating {len(num_features)} individual trend plots...\")\n",
        "            saved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            for feature in num_features:\n",
        "                fig = None\n",
        "                try:\n",
        "                    # Get data for this feature\n",
        "                    feature_data = monthly_psi_df[monthly_psi_df['feature'] == feature].sort_values('month')\n",
        "                    \n",
        "                    if len(feature_data) > 0:\n",
        "                        # Create individual figure for each feature\n",
        "                        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "                        \n",
        "                        # Plot trend\n",
        "                        ax.plot(feature_data['month'], feature_data['psi'], \n",
        "                               marker='o', linewidth=2, markersize=6, color='steelblue')\n",
        "                        \n",
        "                        # Add threshold lines\n",
        "                        ax.axhline(y=PSI_THRESHOLD_MODERATE, color='orange', \n",
        "                                  linestyle='--', linewidth=1.5, label='Moderate (0.1)')\n",
        "                        ax.axhline(y=PSI_THRESHOLD_SIGNIFICANT, color='red', \n",
        "                                  linestyle='--', linewidth=1.5, label='Significant (0.25)')\n",
        "                        \n",
        "                        # Style the plot\n",
        "                        ax.set_title(f'Monthly PSI Trend: {feature}\\n({table_folder_name})', \n",
        "                                   fontsize=12, fontweight='bold')\n",
        "                        ax.set_ylabel('PSI', fontsize=10)\n",
        "                        ax.set_xlabel('Month', fontsize=10)\n",
        "                        ax.legend(fontsize=9, loc='best')\n",
        "                        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "                        ax.tick_params(axis='x', rotation=45)\n",
        "                        \n",
        "                        plt.tight_layout()\n",
        "                        \n",
        "                        # Save individual plot (no display - saved directly to ADLS)\n",
        "                        plot_file = f\"{table_trend_folder}{feature}.png\"\n",
        "                        save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "                        plt.close(fig)  # Explicitly close to free memory\n",
        "                        fig = None  # Prevent double-close\n",
        "                        saved_count += 1\n",
        "                    else:\n",
        "                        if fig is not None:\n",
        "                            plt.close(fig)\n",
        "                        failed_count += 1\n",
        "                        print(f\"    Warning: Skipped {feature} (no data)\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    if fig is not None:\n",
        "                        plt.close(fig)\n",
        "                    failed_count += 1\n",
        "                    print(f\"    Warning: Could not plot {feature}: {str(e)}\")\n",
        "            \n",
        "            print(f\"  âœ“ Monthly trend plots saved: {saved_count} successful, {failed_count} failed\")\n",
        "            print(f\"    Location: {table_trend_folder}\")\n",
        "        \n",
        "        del df, df_intime, df_oot, df_spark\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing {fam_name}-{table}: {str(e)}\")\n",
        "        try:\n",
        "            del df_spark\n",
        "            gc.collect()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"\\nâœ“ Monthly PSI trend analysis complete\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
