{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Target Analysis by Split and Effective Month (Pandas Version)\n",
        "\n",
        "## Overview\n",
        "This notebook analyzes all 33 prediction head targets using **pandas** for data processing.\n",
        "\n",
        "### Pandas Optimizations for Large-Scale Data:\n",
        "- **Chunked processing**: Load and process one chunk at a time\n",
        "- **Incremental aggregation**: Build statistics incrementally\n",
        "- **Memory management**: Explicit garbage collection after each chunk\n",
        "- **Selective column loading**: Only load target columns needed\n",
        "\n",
        "### Analysis Goals:\n",
        "1. Per-head positive count and positive rate by split and month\n",
        "2. Temporal trends in target distributions\n",
        "3. Class imbalance assessment\n",
        "4. Comparison across splits\n",
        "\n",
        "### Outputs:\n",
        "- Target statistics CSV files (by split, by month)\n",
        "- Heatmaps and trend visualizations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pandas==2 -i https://repo.td.com/repository/pypi-all/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"âœ“ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"âœ“ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    import tempfile, os\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    print(f\"âœ“ Saved plot to {adls_path}\")\n",
        "\n",
        "print(\"âœ“ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "TARGET_TRAIN_VAL_PATH = DATA_PATH + \"target/cust/all_products_chunk_320/train_val/\"\n",
        "TARGET_TEST_PATH = DATA_PATH + \"target/cust/all_products_chunk_320/test/\"\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/target_analysis/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "\n",
        "TOTAL_CHUNKS = 320\n",
        "OOT_START_DATE = '2024-01-01'\n",
        "SAMPLING_RATIO = 1.0\n",
        "\n",
        "# All 33 targets\n",
        "TARGETS = [\n",
        "    'cc_acq_2-3', 'cc-aei_acq_2-3', 'cc-aeip_acq_2-3', 'cc-aep_acq_2-3',\n",
        "    'cc-cbe_acq_2-3', 'cc-cbi_acq_2-3', 'cc-fct_acq_2-3', 'cc-lr_acq_2-3',\n",
        "    'cc-pt_acq_2-3', 'cc-rew_acq_2-3', 'cc_att_2-3', 'cc_boat_2-3',\n",
        "    'cc_clip_2-3', 'cc_dg_2-3', 'cc_pap_2-3',\n",
        "    'cc_2ndary_2-3', 'cc_upg_2-3',\n",
        "    'ofi_resl_acq_4-5', 'resl_acq_4-5', 'heloc_att_2-7', 'heloc_cpr_2-3',\n",
        "    'mtg_att_2-7', 'mtg_cpr_2-3',\n",
        "    'ulon_acq_2-3', 'ulon-dcl_acq_2-3', 'ulon-mpl_acq_2-3', 'ulon-rsp_acq_2-3',\n",
        "    'ulon_att_2-3', 'ulon_cpr_2-3',\n",
        "    'uloc_acq_2-3', 'uloc_att_2-3', 'uloc_clip_2-3', 'uloc_cpr_2-3'\n",
        "]\n",
        "\n",
        "TARGET_CATEGORIES = {\n",
        "    'Cards': TARGETS[:17],\n",
        "    'RESL': TARGETS[17:23],\n",
        "    'Unsecured_Loan': TARGETS[23:29],\n",
        "    'Unsecured_LoC': TARGETS[29:33]\n",
        "}\n",
        "\n",
        "def get_category(target):\n",
        "    for cat, targets in TARGET_CATEGORIES.items():\n",
        "        if target in targets:\n",
        "            return cat\n",
        "    return 'Other'\n",
        "\n",
        "print(\"âœ“ Configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Strategy: True Incremental (Constant Memory)\n",
        "\n",
        "### Why This Approach?\n",
        "This notebook uses **true incremental chunked processing** because target statistics (positive counts, positive rates) CAN be aggregated incrementally.\n",
        "\n",
        "### Memory Efficiency:\n",
        "- **Memory usage**: ~800 MB constant (regardless of SAMPLING_RATIO)\n",
        "- **Safe for 100% sampling**: Yes - memory doesn't increase with more data\n",
        "- **Processing**: One chunk at a time, building statistics incrementally\n",
        "\n",
        "### How It Works:\n",
        "```\n",
        "Initialize: target_stats = defaultdict(dict)\n",
        "\n",
        "For each chunk (0 to 319):\n",
        "  1. Load ['pid', 'pred_dt'] + all 33 target columns\n",
        "  2. For each target and split:\n",
        "     - Count non-null values for this target (target.notna().sum())\n",
        "     - Count positive values (target.sum())\n",
        "     - Increment non_null_count\n",
        "     - Increment positive sum\n",
        "  3. Free memory (del df; gc.collect())\n",
        "  4. Move to next chunk\n",
        "\n",
        "After all chunks:\n",
        "  Calculate positive_rate = total_positives / total_non_null_count\n",
        "```\n",
        "\n",
        "### Why Incremental Works Here:\n",
        "- **Non-null counts**: sum(chunk1_non_null) + sum(chunk2_non_null) = total_non_null âœ“\n",
        "- **Sums**: sum(chunk1_positives) + sum(chunk2_positives) = total_positives âœ“\n",
        "- **Rates**: Calculated at the end from aggregated non-null counts and sums âœ“\n",
        "- **Important**: Only non-null values are counted in denominator (handles missing target values)\n",
        "\n",
        "### Key Fix:\n",
        "- **Positive rate calculation**: `positive / non_null_count` (not `positive / total_samples`)\n",
        "- Some targets have null values for certain samples - these are excluded from rate calculation\n",
        "- This ensures accurate positive rates that reflect the true proportion among valid observations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Process Targets Chunk by Chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PROCESSING TARGET DATA CHUNK BY CHUNK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Incremental statistics\n",
        "target_stats_by_split = defaultdict(lambda: defaultdict(lambda: {'count': 0, 'positive': 0}))\n",
        "target_stats_by_month = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {'count': 0, 'positive': 0})))\n",
        "\n",
        "cols_to_load = ['pid', 'pred_dt'] + [t for t in TARGETS]\n",
        "\n",
        "# Process both train_val and test\n",
        "for source, base_path in [('train_val', TARGET_TRAIN_VAL_PATH), ('test', TARGET_TEST_PATH)]:\n",
        "    print(f\"\\nProcessing {source} chunks...\")\n",
        "    \n",
        "    for chunk_id in range(TOTAL_CHUNKS):\n",
        "        chunk_path = f\"{base_path}chunk={chunk_id}/\"\n",
        "        \n",
        "        try:\n",
        "            files = [f.path for f in dbutils.fs.ls(chunk_path) if f.name.endswith('.csv')]\n",
        "            if not files:\n",
        "                continue\n",
        "            \n",
        "            for file_path in files:\n",
        "                import tempfile, os\n",
        "                with tempfile.NamedTemporaryFile(mode='wb', suffix='.csv', delete=False) as tmp:\n",
        "                    tmp_path = tmp.name\n",
        "                dbutils.fs.cp(file_path, f\"file:{tmp_path}\")\n",
        "                \n",
        "                # Load only necessary columns\n",
        "                available_cols = pd.read_csv(tmp_path, nrows=0).columns.tolist()\n",
        "                cols_load = [c for c in cols_to_load if c in available_cols]\n",
        "                df = pd.read_csv(tmp_path, usecols=cols_load)\n",
        "                os.remove(tmp_path)\n",
        "                \n",
        "                if SAMPLING_RATIO < 1.0:\n",
        "                    df = df.sample(frac=SAMPLING_RATIO, random_state=42)\n",
        "                \n",
        "                df = df.rename(columns={'pid': 'cust_id', 'pred_dt': 'efectv_dt'})\n",
        "                \n",
        "                # Ensure efectv_dt is datetime and convert OOT_START_DATE for comparison\n",
        "                if 'efectv_dt' in df.columns:\n",
        "                    df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "                    oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "                    df['split_label'] = df.apply(\n",
        "                        lambda row: 'OOT' if row['efectv_dt'] >= oot_date\n",
        "                        else 'train' if chunk_id < 256 else 'valid',\n",
        "                        axis=1\n",
        "                    )\n",
        "                else:\n",
        "                    df['split_label'] = df.apply(\n",
        "                        lambda row: 'train' if chunk_id < 256 else 'valid',\n",
        "                        axis=1\n",
        "                    )\n",
        "                \n",
        "                # Aggregate statistics\n",
        "                for target in TARGETS:\n",
        "                    if target in df.columns:\n",
        "                        # By split\n",
        "                        for split in df['split_label'].unique():\n",
        "                            split_data = df[df['split_label'] == split]\n",
        "                            # Count non-null values for this target (exclude nulls from denominator)\n",
        "                            non_null_count = split_data[target].notna().sum()\n",
        "                            positive_count = split_data[target].sum()  # sum() treats NaN as 0, but we only count non-null\n",
        "                            # Only count positive from non-null values\n",
        "                            if non_null_count > 0:\n",
        "                                target_stats_by_split[split][target]['count'] += non_null_count\n",
        "                                target_stats_by_split[split][target]['positive'] += positive_count\n",
        "                        \n",
        "                        # By month and split\n",
        "                        for (date, split), group in df.groupby(['efectv_dt', 'split_label']):\n",
        "                            # Count non-null values for this target (exclude nulls from denominator)\n",
        "                            non_null_count = group[target].notna().sum()\n",
        "                            positive_count = group[target].sum()  # sum() treats NaN as 0, but we only count non-null\n",
        "                            # Only count positive from non-null values\n",
        "                            if non_null_count > 0:\n",
        "                                target_stats_by_month[date][split][target]['count'] += non_null_count\n",
        "                                target_stats_by_month[date][split][target]['positive'] += positive_count\n",
        "                \n",
        "                del df\n",
        "                gc.collect()\n",
        "            \n",
        "            if (chunk_id + 1) % 50 == 0:\n",
        "                print(f\"  Processed {chunk_id + 1}/{TOTAL_CHUNKS} chunks...\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "print(\"\\nâœ“ All chunks processed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CREATING SUMMARY DATAFRAMES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Results by split\n",
        "# Note: 'count' now represents non-null count (not total samples)\n",
        "results_by_split = []\n",
        "for split, targets_dict in target_stats_by_split.items():\n",
        "    for target, stats in targets_dict.items():\n",
        "        non_null_count = stats['count']  # This is now the non-null count\n",
        "        positive_count = int(stats['positive'])\n",
        "        positive_rate = positive_count / non_null_count if non_null_count > 0 else 0\n",
        "        results_by_split.append({\n",
        "            'split': split,\n",
        "            'target': target,\n",
        "            'non_null_count': non_null_count,  # Renamed for clarity\n",
        "            'positive_count': positive_count,\n",
        "            'positive_rate': positive_rate,  # positive / non_null_count (excludes nulls)\n",
        "            'category': get_category(target)\n",
        "        })\n",
        "\n",
        "results_by_split_df = pd.DataFrame(results_by_split)\n",
        "save_pandas_to_csv_adls(results_by_split_df, OUTPUT_PATH + \"target_statistics_by_split.csv\")\n",
        "\n",
        "# Results by month\n",
        "# Note: 'count' now represents non-null count (not total samples)\n",
        "results_by_month = []\n",
        "for date, splits_dict in target_stats_by_month.items():\n",
        "    for split, targets_dict in splits_dict.items():\n",
        "        for target, stats in targets_dict.items():\n",
        "            non_null_count = stats['count']  # This is now the non-null count\n",
        "            positive_count = int(stats['positive'])\n",
        "            positive_rate = positive_count / non_null_count if non_null_count > 0 else 0\n",
        "            results_by_month.append({\n",
        "                'efectv_dt': date,\n",
        "                'split': split,\n",
        "                'target': target,\n",
        "                'non_null_count': non_null_count,  # Renamed for clarity\n",
        "                'positive_count': positive_count,\n",
        "                'positive_rate': positive_rate,  # positive / non_null_count (excludes nulls)\n",
        "                'category': get_category(target)\n",
        "            })\n",
        "\n",
        "results_by_month_df = pd.DataFrame(results_by_month)\n",
        "save_pandas_to_csv_adls(results_by_month_df, OUTPUT_PATH + \"target_statistics_by_month.csv\")\n",
        "\n",
        "print(\"âœ“ DataFrames created and saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations and Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Criteria and Key Findings\n",
        "\n",
        "### âœ… **Validation Passes If**:\n",
        "- All 33 targets have statistics calculated\n",
        "- Positive rates are reasonable (not all 0% or 100%)\n",
        "- Class imbalance is quantified for each target\n",
        "- Temporal trends show stability (no sudden jumps)\n",
        "- All splits (train/valid/OOT) have data\n",
        "\n",
        "### ðŸ“Š **Key Metrics to Report**:\n",
        "After running this notebook, document:\n",
        "- **Highly imbalanced targets** (<1% positive rate): Count and list\n",
        "- **Moderately imbalanced targets** (1-5% positive rate): Count and list\n",
        "- **Distribution shifts**: Targets with >0.1pp difference between train and OOT\n",
        "- **Temporal anomalies**: Any targets with sudden rate changes over time\n",
        "\n",
        "### ðŸ“ˆ **Expected Findings**:\n",
        "- Majority of targets will be highly imbalanced (<1% positive rate) - this is normal\n",
        "- Cards targets typically have higher positive rates than RESL/ULON targets\n",
        "- Sub-product targets (cc-aei, cc-cbe, etc.) usually have lower rates than core products\n",
        "- OOT positive rates may differ slightly from in-time (temporal drift)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class imbalance analysis\n",
        "imbalance_pivot = results_by_split_df.pivot(index='target', columns='split', values='positive_rate')\n",
        "imbalance_pivot = imbalance_pivot[['train', 'valid', 'OOT']]\n",
        "imbalance_pivot['category'] = imbalance_pivot.index.map(get_category)\n",
        "imbalance_pivot['mean_positive_rate'] = imbalance_pivot[['train', 'valid', 'OOT']].mean(axis=1)\n",
        "imbalance_pivot = imbalance_pivot.sort_values('mean_positive_rate')\n",
        "save_pandas_to_csv_adls(imbalance_pivot, OUTPUT_PATH + \"class_imbalance_summary.csv\")\n",
        "\n",
        "# Create split comparison analysis\n",
        "comparison = imbalance_pivot[['train', 'valid', 'OOT', 'category']].copy()\n",
        "comparison['train_vs_valid_diff'] = comparison['valid'] - comparison['train']\n",
        "comparison['train_vs_oot_diff'] = comparison['OOT'] - comparison['train']\n",
        "comparison['valid_vs_oot_diff'] = comparison['OOT'] - comparison['valid']\n",
        "comparison['train_vs_valid_pct'] = (comparison['train_vs_valid_diff'] / comparison['train']) * 100\n",
        "comparison['train_vs_oot_pct'] = (comparison['train_vs_oot_diff'] / comparison['train']) * 100\n",
        "comparison['valid_vs_oot_pct'] = (comparison['valid_vs_oot_diff'] / comparison['valid']) * 100\n",
        "save_pandas_to_csv_adls(comparison.reset_index(), OUTPUT_PATH + \"split_comparison_analysis.csv\")\n",
        "\n",
        "# Heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 18))\n",
        "heatmap_matrix = imbalance_pivot[['train', 'valid', 'OOT']]\n",
        "sns.heatmap(heatmap_matrix, annot=True, fmt='.4f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Positive Rate'}, ax=ax, linewidths=0.5)\n",
        "ax.set_title('Target Positive Rates by Split', fontsize=16, fontweight='bold', pad=20)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9)\n",
        "plt.tight_layout()\n",
        "save_plot_to_adls(fig, OUTPUT_PATH + \"positive_rate_heatmap_by_split.png\", dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# Temporal trends by category\n",
        "for category, targets in TARGET_CATEGORIES.items():\n",
        "    cat_data = results_by_month_df[results_by_month_df['target'].isin(targets)]\n",
        "    if len(cat_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "    for idx, split in enumerate(['train', 'valid', 'OOT']):\n",
        "        split_data = cat_data[cat_data['split'] == split]\n",
        "        for target in targets:\n",
        "            target_data = split_data[split_data['target'] == target].sort_values('efectv_dt')\n",
        "            if len(target_data) > 0:\n",
        "                axes[idx].plot(target_data['efectv_dt'], target_data['positive_rate'], \n",
        "                              marker='o', label=target, linewidth=2, markersize=4, alpha=0.7)\n",
        "        axes[idx].set_xlabel('Effective Date', fontsize=11)\n",
        "        axes[idx].set_ylabel('Positive Rate', fontsize=11)\n",
        "        axes[idx].set_title(f'{category} - {split.upper()} Split', fontsize=12, fontweight='bold')\n",
        "        axes[idx].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "    plt.tight_layout()\n",
        "    save_plot_to_adls(fig, OUTPUT_PATH + f\"temporal_trends_{category}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "print(\"âœ“ Visualizations complete\")\n",
        "print(\"âœ“ Target analysis complete\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
