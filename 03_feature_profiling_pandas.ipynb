{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Profiling by Table (Pandas Version)\n",
        "\n",
        "## Overview\n",
        "Comprehensive feature profiling using **pandas** with memory-efficient chunk processing.\n",
        "\n",
        "### Pandas Optimizations:\n",
        "- **Chunked reading**: Process data in manageable chunks\n",
        "- **Streaming statistics**: Calculate stats without loading full table\n",
        "- **Memory efficient**: Use pandas iterators and explicit cleanup\n",
        "\n",
        "### Statistics Calculated:\n",
        "- Data type, % zeros, n_unique\n",
        "- Most frequent value and percentage\n",
        "- Percentiles: min, 1%, 50%, 99%, max, mean\n",
        "\n",
        "### Outputs:\n",
        "- Feature profiling CSVs per table with **separate statistics for In-Time vs OOT**\n",
        "  - Each feature has two rows: one for 'In-Time' period, one for 'OOT' period\n",
        "  - Includes `time_period` column to distinguish periods\n",
        "- Individual boxplots for each feature (one PNG per feature)\n",
        "  - Saved in table-specific folders: `plots/{table_name}/`\n",
        "  - Each file named: `{feature_name}.png`\n",
        "  - Comparing OOT vs in-time distributions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pandas==2 -i https://repo.td.com/repository/pypi-all/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import gc\n",
        "\n",
        "# Helper functions\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"✓ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    import tempfile, os\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    print(f\"✓ Saved plot to {adls_path}\")\n",
        "\n",
        "print(\"✓ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/feature_profiling/\"\n",
        "PLOT_PATH = OUTPUT_PATH + \"plots/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "dbutils.fs.mkdirs(PLOT_PATH)\n",
        "\n",
        "SAMPLING_RATIO = 0.01\n",
        "PLOT_SAMPLING_RATIO = 0.01\n",
        "OOT_START_DATE = '2024-01-01'\n",
        "\n",
        "# Feature tables to analyze\n",
        "TABLES = [\n",
        "    (\"cust\", \"cust_basic_sumary\", ''),\n",
        "    (\"cust\", \"batch_credit_bureau\", ''),\n",
        "    (\"dem\", \"acct\", 2438),\n",
        "    (\"cc\", \"acct\", 2444),\n",
        "    (\"loc\", \"acct\", 2442),\n",
        "    (\"loan\", \"acct\", 2439),\n",
        "    (\"mtg\", \"acct\", 2440),\n",
        "    (\"inv\", \"acct\", 1331),\n",
        "    (\"dem\", \"acct_trans\", 2438),\n",
        "    (\"cc\", \"acct_trans\", 2444),\n",
        "]\n",
        "\n",
        "# Load metadata\n",
        "feature_metadata_rows = spark.read.text(f\"{DATA_PATH}/feature/feature_metadata.jsonl\").collect()\n",
        "feature_metadata = json.loads('\\n'.join([row.value for row in feature_metadata_rows]))\n",
        "\n",
        "print(\"✓ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Strategy: Sampled Full-Table (Accuracy Prioritized)\n",
        "\n",
        "### Why This Approach?\n",
        "This notebook calculates **median, percentiles (p1, p99)** which **CANNOT be calculated incrementally**. We must see all values to sort/rank them accurately.\n",
        "\n",
        "### Memory Efficiency:\n",
        "- **Memory usage**: Scales with SAMPLING_RATIO\n",
        "- **Mitigation**: Process one table at a time (10 tables total), free memory between tables\n",
        "- **Recommendation for memory issue**: Use `SAMPLING_RATIO = 0.01` (1%) for accurate results with manageable memory\n",
        "\n",
        "### How It Works:\n",
        "```\n",
        "For each table (10 total):\n",
        "  1. Load FULL table via Spark (efficient Parquet reading)\n",
        "  2. Apply sampling at Spark level: .sample(fraction=SAMPLING_RATIO)\n",
        "  3. Convert to pandas: .toPandas()\n",
        "  4. Calculate accurate statistics:\n",
        "     - median: df[col].median() ← requires sorted values\n",
        "     - p99: df[col].quantile(0.99) ← requires percentile calculation\n",
        "     - mean, min, max, n_unique, etc.\n",
        "  5. Free memory before next table (del df; gc.collect())\n",
        "```\n",
        "\n",
        "### Why Incremental Doesn't Work Here:\n",
        "- ❌ median(chunk1) + median(chunk2) ≠ median(all_data)\n",
        "- ❌ p99(chunk1) combined with p99(chunk2) ≠ p99(all_data)\n",
        "- ✅ Must see all sampled values together to calculate correct percentiles\n",
        "- ✅ 1% sampling gives exact statistics on representative sample\n",
        "\n",
        "### Alternative Considered:\n",
        "Could use approximate algorithms (T-Digest, Q-Digest) for streaming percentiles, but:\n",
        "- ❌ Introduces approximation error\n",
        "- ❌ Complex to implement and debug\n",
        "- ✅ 1% sampling gives exact results with manageable memory\n",
        "- ✅ Simpler code is easier to maintain\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Criteria and Expected Results\n",
        "\n",
        "### ✅ **Profiling Succeeds If**:\n",
        "- All tables processed successfully\n",
        "- Statistics calculated for **all features** (numerical + categorical) in metadata\n",
        "- **Separate statistics** calculated for In-Time vs OOT periods\n",
        "- No excessive missing values (>99.9%) unless expected\n",
        "- Reasonable value ranges (no extreme outliers unless business-valid)\n",
        "- Categorical features have reasonable cardinality\n",
        "- **Time-period comparison** shows expected differences between In-Time and OOT\n",
        "\n",
        "### 📊 **Statistics Calculated Per Feature (Per Time Period)**:\n",
        "| Statistic | Numerical | Categorical | Notes |\n",
        "|-----------|-----------|-------------|-------|\n",
        "| time_period | ✓ | ✓ | 'In-Time' or 'OOT' |\n",
        "| feature | ✓ | ✓ | Feature name |\n",
        "| data_type | ✓ | ✓ | Identifies feature type |\n",
        "| pct_zero | ✓ | ✓ | % of values that are 0 |\n",
        "| n_unique | ✓ | ✓ | Number of distinct values |\n",
        "| most_frequent_value | ✓ | ✓ | Mode |\n",
        "| pct_most_frequent | ✓ | ✓ | % of samples with mode |\n",
        "| min | ✓ | ✓ | Minimum value |\n",
        "| max | ✓ | ✓ | Maximum value |\n",
        "| p1 | ✓ | ✗ | 1st percentile |\n",
        "| median (p50) | ✓ | ✗ | 50th percentile |\n",
        "| p99 | ✓ | ✗ | 99th percentile |\n",
        "| mean | ✓ | ✗ | Average value |\n",
        "\n",
        "### 📈 **Time-Period Analysis**:\n",
        "Each feature has **two rows** in the output CSV:\n",
        "- **Row 1**: Statistics for 'In-Time' period (before 2024-01-01)\n",
        "- **Row 2**: Statistics for 'OOT' period (2024-01-01 and after)\n",
        "\n",
        "**What to Compare**:\n",
        "- **Distributions**: Compare median, mean, percentiles between periods\n",
        "- **Sparsity**: Compare pct_zero (may increase/decrease over time)\n",
        "- **Cardinality**: Compare n_unique for categorical features\n",
        "- **Ranges**: Compare min/max values (may indicate data quality issues)\n",
        "- **Mode**: Compare most_frequent_value (distribution shifts)\n",
        "\n",
        "### ⚠️ **Potential Issues to Flag**:\n",
        "- **Features with >99% zeros** (may be redundant or sparse)\n",
        "- **Features with only 1 unique value** (constant features - no information)\n",
        "- **Features with extreme ranges** (may need normalization or clipping)\n",
        "- **Categorical features with very high cardinality** (>1000 categories - may need bucketing)\n",
        "- **Features missing from certain tables** (expected for table-specific features)\n",
        "- **Large differences between In-Time and OOT**:\n",
        "  - Significant shifts in median/mean (potential drift)\n",
        "  - Large changes in pct_zero (sparsity changes)\n",
        "  - Cardinality changes in categoricals (new categories appear/disappear)\n",
        "\n",
        "### 📊 **Boxplot Visualizations**:\n",
        "- **One boxplot per numerical feature** comparing In-Time vs OOT\n",
        "- Saved individually: `plots/{table_name}/{feature_name}.png`\n",
        "- **What to look for**:\n",
        "  - Distribution shifts (boxes at different positions)\n",
        "  - Spread changes (box width differences)\n",
        "  - Outlier patterns (fliers in different locations)\n",
        "  - Median differences (horizontal line position)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE PROFILING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        continue\n",
        "    \n",
        "    num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "    cat_features = list(feature_metadata[fam_name][table_meta_key].get(\"cat_features\", {}).keys())\n",
        "    \n",
        "    # Load via spark then convert to pandas (more efficient for parquet)\n",
        "    df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "    if SAMPLING_RATIO < 1.0:\n",
        "        df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    \n",
        "    # Load with efectv_dt if available for time-period splitting\n",
        "    cols_to_load = [c for c in (num_features + cat_features) if c in df_spark.columns]\n",
        "    if 'efectv_dt' in df_spark.columns:\n",
        "        cols_to_load = ['efectv_dt'] + cols_to_load\n",
        "    \n",
        "    df = df_spark.select(cols_to_load).toPandas()\n",
        "    \n",
        "    # Split into In-Time and OOT periods if efectv_dt is available\n",
        "    if 'efectv_dt' in df.columns:\n",
        "        df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "        df['time_period'] = df['efectv_dt'].apply(\n",
        "            lambda x: 'OOT' if x >= pd.to_datetime(OOT_START_DATE) else 'In-Time'\n",
        "        )\n",
        "        df_intime = df[df['time_period'] == 'In-Time'].copy()\n",
        "        df_oot = df[df['time_period'] == 'OOT'].copy()\n",
        "        \n",
        "        # Profile features separately for In-Time and OOT\n",
        "        all_stats = []\n",
        "        \n",
        "        for period_name, period_df in [('In-Time', df_intime), ('OOT', df_oot)]:\n",
        "            for feature in num_features + cat_features:\n",
        "                if feature not in period_df.columns:\n",
        "                    continue\n",
        "                is_cat = feature in cat_features\n",
        "                \n",
        "                # Skip if no data in this period\n",
        "                if len(period_df) == 0:\n",
        "                    continue\n",
        "                \n",
        "                stats = {\n",
        "                    'time_period': period_name,\n",
        "                    'feature': feature,\n",
        "                    'data_type': 'categorical' if is_cat else 'numerical',\n",
        "                    'pct_zero': (period_df[feature] == 0).mean() if len(period_df[feature].dropna()) > 0 else None,\n",
        "                    'n_unique': period_df[feature].nunique(),\n",
        "                    'most_frequent_value': period_df[feature].mode()[0] if len(period_df[feature].mode()) > 0 else None,\n",
        "                    'pct_most_frequent': period_df[feature].value_counts(normalize=True).iloc[0] if len(period_df[feature].dropna()) > 0 else None,\n",
        "                }\n",
        "                \n",
        "                if not is_cat:\n",
        "                    feature_clean = period_df[feature].dropna()\n",
        "                    if len(feature_clean) > 0:\n",
        "                        # Convert to float to handle Decimal types\n",
        "                        feature_clean = pd.to_numeric(feature_clean, errors='coerce').dropna()\n",
        "                        if len(feature_clean) > 0:\n",
        "                            stats.update({\n",
        "                                'min': float(feature_clean.min()),\n",
        "                                'p1': float(feature_clean.quantile(0.01)),\n",
        "                                'median': float(feature_clean.median()),\n",
        "                                'p99': float(feature_clean.quantile(0.99)),\n",
        "                                'max': float(feature_clean.max()),\n",
        "                                'mean': float(feature_clean.mean())\n",
        "                            })\n",
        "                        else:\n",
        "                            stats.update({\n",
        "                                'min': None, 'p1': None, 'median': None,\n",
        "                                'p99': None, 'max': None, 'mean': None\n",
        "                            })\n",
        "                    else:\n",
        "                        stats.update({\n",
        "                            'min': None, 'p1': None, 'median': None,\n",
        "                            'p99': None, 'max': None, 'mean': None\n",
        "                        })\n",
        "                else:\n",
        "                    feature_clean = period_df[feature].dropna()\n",
        "                    if len(feature_clean) > 0:\n",
        "                        # Convert to numeric for min/max to handle Decimal types\n",
        "                        try:\n",
        "                            min_val = feature_clean.min()\n",
        "                            max_val = feature_clean.max()\n",
        "                            # Try to convert to float if possible\n",
        "                            if hasattr(min_val, '__float__'):\n",
        "                                min_val = float(min_val)\n",
        "                            if hasattr(max_val, '__float__'):\n",
        "                                max_val = float(max_val)\n",
        "                            stats.update({\n",
        "                                'min': min_val,\n",
        "                                'max': max_val,\n",
        "                                'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "                            })\n",
        "                        except:\n",
        "                            stats.update({\n",
        "                                'min': str(min_val) if 'min_val' in locals() else None,\n",
        "                                'max': str(max_val) if 'max_val' in locals() else None,\n",
        "                                'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "                            })\n",
        "                    else:\n",
        "                        stats.update({\n",
        "                            'min': None, 'max': None,\n",
        "                            'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "                        })\n",
        "                \n",
        "                all_stats.append(stats)\n",
        "        \n",
        "        # Save profiling results with time_period column\n",
        "        if all_stats:\n",
        "            results_df = pd.DataFrame(all_stats)\n",
        "            # Reorder columns: time_period first, then feature, then rest\n",
        "            col_order = ['time_period', 'feature', 'data_type'] + [c for c in results_df.columns if c not in ['time_period', 'feature', 'data_type']]\n",
        "            results_df = results_df[col_order]\n",
        "            save_pandas_to_csv_adls(results_df, f\"{OUTPUT_PATH}feature_profile_{fam_name}_{table}.csv\")\n",
        "    else:\n",
        "        # No efectv_dt column - profile on all data together (fallback)\n",
        "        all_stats = []\n",
        "        for feature in num_features + cat_features:\n",
        "            if feature not in df.columns:\n",
        "                continue\n",
        "            is_cat = feature in cat_features\n",
        "            stats = {\n",
        "                'time_period': 'All',\n",
        "                'feature': feature,\n",
        "                'data_type': 'categorical' if is_cat else 'numerical',\n",
        "                'pct_zero': (df[feature] == 0).mean(),\n",
        "                'n_unique': df[feature].nunique(),\n",
        "                'most_frequent_value': df[feature].mode()[0] if len(df[feature].mode()) > 0 else None,\n",
        "                'pct_most_frequent': df[feature].value_counts(normalize=True).iloc[0] if len(df) > 0 else None,\n",
        "            }\n",
        "            if not is_cat:\n",
        "                # Convert to float to handle Decimal types\n",
        "                feature_data = pd.to_numeric(df[feature], errors='coerce').dropna()\n",
        "                if len(feature_data) > 0:\n",
        "                    stats.update({\n",
        "                        'min': float(feature_data.min()),\n",
        "                        'p1': float(feature_data.quantile(0.01)),\n",
        "                        'median': float(feature_data.median()),\n",
        "                        'p99': float(feature_data.quantile(0.99)),\n",
        "                        'max': float(feature_data.max()),\n",
        "                        'mean': float(feature_data.mean())\n",
        "                    })\n",
        "                else:\n",
        "                    stats.update({\n",
        "                        'min': None, 'p1': None, 'median': None,\n",
        "                        'p99': None, 'max': None, 'mean': None\n",
        "                    })\n",
        "            else:\n",
        "                # For categorical, convert min/max to handle Decimal types\n",
        "                try:\n",
        "                    min_val = df[feature].min()\n",
        "                    max_val = df[feature].max()\n",
        "                    # Try to convert to float if possible\n",
        "                    if hasattr(min_val, '__float__'):\n",
        "                        min_val = float(min_val)\n",
        "                    if hasattr(max_val, '__float__'):\n",
        "                        max_val = float(max_val)\n",
        "                    stats.update({\n",
        "                        'min': min_val,\n",
        "                        'max': max_val,\n",
        "                        'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "                    })\n",
        "                except:\n",
        "                    stats.update({\n",
        "                        'min': str(df[feature].min()) if len(df[feature].dropna()) > 0 else None,\n",
        "                        'max': str(df[feature].max()) if len(df[feature].dropna()) > 0 else None,\n",
        "                        'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "                    })\n",
        "            all_stats.append(stats)\n",
        "        \n",
        "        # Save profiling results\n",
        "        if all_stats:\n",
        "            results_df = pd.DataFrame(all_stats)\n",
        "            col_order = ['time_period', 'feature', 'data_type'] + [c for c in results_df.columns if c not in ['time_period', 'feature', 'data_type']]\n",
        "            results_df = results_df[col_order]\n",
        "            save_pandas_to_csv_adls(results_df, f\"{OUTPUT_PATH}feature_profile_{fam_name}_{table}.csv\")\n",
        "    \n",
        "    # Create boxplots (OOT vs In-Time comparison) - One plot per feature\n",
        "    if 'efectv_dt' in df.columns:\n",
        "        # Ensure efectv_dt is datetime and convert OOT_START_DATE for comparison\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['efectv_dt']):\n",
        "            df['efectv_dt'] = pd.to_datetime(df['efectv_dt'])\n",
        "        oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "        df['time_period'] = df['efectv_dt'].apply(\n",
        "            lambda x: 'OOT' if x >= oot_date else 'In-Time'\n",
        "        )\n",
        "        \n",
        "        # Create folder for this table's plots\n",
        "        table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "        table_plot_folder = f\"{PLOT_PATH}{table_folder_name}/\"\n",
        "        dbutils.fs.mkdirs(table_plot_folder)\n",
        "        \n",
        "        # Get ALL numerical features for plotting\n",
        "        plot_features = [f for f in num_features if f in df.columns]\n",
        "        \n",
        "        if len(plot_features) > 0:\n",
        "            print(f\"  Creating {len(plot_features)} individual boxplots...\")\n",
        "            saved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            for feature in plot_features:\n",
        "                fig = None\n",
        "                try:\n",
        "                    # Create individual figure for each feature\n",
        "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                    \n",
        "                    # Prepare data for boxplot\n",
        "                    intime_data = df[df['time_period'] == 'In-Time'][feature].dropna()\n",
        "                    oot_data = df[df['time_period'] == 'OOT'][feature].dropna()\n",
        "                    \n",
        "                    if len(intime_data) > 0 and len(oot_data) > 0:\n",
        "                        # Create boxplot data structure\n",
        "                        plot_data = {\n",
        "                            'In-Time': intime_data,\n",
        "                            'OOT': oot_data\n",
        "                        }\n",
        "                        \n",
        "                        # Create boxplot\n",
        "                        bp = ax.boxplot([plot_data['In-Time'], plot_data['OOT']], \n",
        "                                       labels=['In-Time', 'OOT'], \n",
        "                                       vert=True, patch_artist=True,\n",
        "                                       showmeans=False, showfliers=True)\n",
        "                        \n",
        "                        # Style the boxes with colors\n",
        "                        colors = ['lightblue', 'lightcoral']\n",
        "                        for patch, color in zip(bp['boxes'], colors):\n",
        "                            patch.set_facecolor(color)\n",
        "                            patch.set_alpha(0.6)\n",
        "                        \n",
        "                        ax.set_title(f'{feature}\\n({table_folder_name})', \n",
        "                                   fontsize=12, fontweight='bold')\n",
        "                        ax.set_ylabel('Value', fontsize=10)\n",
        "                        ax.set_xlabel('Time Period', fontsize=10)\n",
        "                        ax.grid(True, alpha=0.2, axis='y', linestyle='--')\n",
        "                        \n",
        "                        plt.tight_layout()\n",
        "                        \n",
        "                        # Save individual plot (no display - saved directly to ADLS)\n",
        "                        plot_file = f\"{table_plot_folder}{feature}.png\"\n",
        "                        save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "                        plt.close(fig)  # Explicitly close to free memory\n",
        "                        fig = None  # Prevent double-close\n",
        "                        saved_count += 1\n",
        "                    else:\n",
        "                        if fig is not None:\n",
        "                            plt.close(fig)\n",
        "                        failed_count += 1\n",
        "                        print(f\"    Warning: Skipped {feature} (insufficient data)\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    if fig is not None:\n",
        "                        plt.close(fig)\n",
        "                    failed_count += 1\n",
        "                    print(f\"    Warning: Could not plot {feature}: {str(e)}\")\n",
        "            \n",
        "            print(f\"  ✓ Boxplots saved: {saved_count} successful, {failed_count} failed\")\n",
        "            print(f\"    Location: {table_plot_folder}\")\n",
        "    \n",
        "    del df, df_spark\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n✓ Feature profiling complete\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
