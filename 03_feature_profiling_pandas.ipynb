{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Profiling by Table (Pandas Version)\n",
        "\n",
        "## Overview\n",
        "Comprehensive feature profiling using **pandas** with memory-efficient chunk processing.\n",
        "\n",
        "### Pandas Optimizations:\n",
        "- **Chunked reading**: Process data in manageable chunks\n",
        "- **Streaming statistics**: Calculate stats without loading full table\n",
        "- **Memory efficient**: Use pandas iterators and explicit cleanup\n",
        "\n",
        "### Statistics Calculated:\n",
        "- Data type, % zeros, n_unique\n",
        "- Most frequent value and percentage\n",
        "- Percentiles: min, 1%, 50%, 99%, max, mean\n",
        "\n",
        "### Outputs:\n",
        "- Feature profiling CSVs per table with **separate statistics for In-Time vs OOT**\n",
        "  - Each feature has two rows: one for 'In-Time' period, one for 'OOT' period\n",
        "  - Includes `time_period` column to distinguish periods\n",
        "- Individual boxplots for each feature (one PNG per feature)\n",
        "  - Saved in table-specific folders: `plots/{table_name}/`\n",
        "  - Each file named: `{feature_name}.png`\n",
        "  - Comparing OOT vs in-time distributions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pandas==2 -i https://repo.td.com/repository/pypi-all/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import gc\n",
        "from joblib import Parallel, delayed\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Helper functions\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"✓ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    import tempfile, os\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    print(f\"✓ Saved plot to {adls_path}\")\n",
        "\n",
        "# Optimized feature profiling function\n",
        "def profile_feature_optimized(feature, period_df, is_cat):\n",
        "    \"\"\"Vectorized feature profiling - much faster than row-by-row\"\"\"\n",
        "    stats = {\n",
        "        'feature': feature,\n",
        "        'data_type': 'categorical' if is_cat else 'numerical',\n",
        "    }\n",
        "    \n",
        "    # Vectorized operations\n",
        "    feature_data = period_df[feature]\n",
        "    feature_clean = feature_data.dropna()\n",
        "    \n",
        "    if len(feature_clean) == 0:\n",
        "        stats.update({\n",
        "            'pct_zero': None, 'n_unique': 0, 'most_frequent_value': None,\n",
        "            'pct_most_frequent': None, 'min': None, 'max': None,\n",
        "            'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "        })\n",
        "        return stats\n",
        "    \n",
        "    stats['pct_zero'] = (feature_data == 0).mean()\n",
        "    stats['n_unique'] = feature_clean.nunique()\n",
        "    \n",
        "    # Most frequent (mode)\n",
        "    value_counts = feature_clean.value_counts()\n",
        "    if len(value_counts) > 0:\n",
        "        stats['most_frequent_value'] = value_counts.index[0]\n",
        "        stats['pct_most_frequent'] = value_counts.iloc[0] / len(feature_clean)\n",
        "    else:\n",
        "        stats['most_frequent_value'] = None\n",
        "        stats['pct_most_frequent'] = None\n",
        "    \n",
        "    if not is_cat:\n",
        "        # Convert to numeric for numerical features\n",
        "        feature_numeric = pd.to_numeric(feature_clean, errors='coerce').dropna()\n",
        "        if len(feature_numeric) > 0:\n",
        "            # Vectorized percentile calculations\n",
        "            percentiles = np.percentile(feature_numeric, [0, 1, 50, 99, 100])\n",
        "            stats.update({\n",
        "                'min': float(percentiles[0]),\n",
        "                'p1': float(percentiles[1]),\n",
        "                'median': float(percentiles[2]),\n",
        "                'p99': float(percentiles[3]),\n",
        "                'max': float(percentiles[4]),\n",
        "                'mean': float(feature_numeric.mean())\n",
        "            })\n",
        "        else:\n",
        "            stats.update({\n",
        "                'min': None, 'p1': None, 'median': None,\n",
        "                'p99': None, 'max': None, 'mean': None\n",
        "            })\n",
        "    else:\n",
        "        # Categorical: only min/max\n",
        "        try:\n",
        "            min_val = feature_clean.min()\n",
        "            max_val = feature_clean.max()\n",
        "            if hasattr(min_val, '__float__'):\n",
        "                min_val = float(min_val)\n",
        "            if hasattr(max_val, '__float__'):\n",
        "                max_val = float(max_val)\n",
        "            stats.update({\n",
        "                'min': min_val, 'max': max_val,\n",
        "                'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "            })\n",
        "        except:\n",
        "            stats.update({\n",
        "                'min': str(feature_clean.min()) if len(feature_clean) > 0 else None,\n",
        "                'max': str(feature_clean.max()) if len(feature_clean) > 0 else None,\n",
        "                'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "            })\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Optimized boxplot creation function\n",
        "def create_boxplot_optimized(feature, intime_data, oot_data, table_folder_name, table_plot_folder):\n",
        "    \"\"\"Create boxplot for a single feature - optimized for parallel processing\"\"\"\n",
        "    try:\n",
        "        # Convert to numeric and numpy arrays (fast)\n",
        "        intime_arr = pd.to_numeric(intime_data, errors='coerce').dropna()\n",
        "        oot_arr = pd.to_numeric(oot_data, errors='coerce').dropna()\n",
        "        \n",
        "        intime_arr = np.array(intime_arr, dtype=np.float64)\n",
        "        oot_arr = np.array(oot_arr, dtype=np.float64)\n",
        "        \n",
        "        intime_arr = intime_arr[np.isfinite(intime_arr)]\n",
        "        oot_arr = oot_arr[np.isfinite(oot_arr)]\n",
        "        \n",
        "        if len(intime_arr) > 0 and len(oot_arr) > 0:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            \n",
        "            # HORIZONTAL boxplot\n",
        "            bp = ax.boxplot([intime_arr, oot_arr], \n",
        "                           labels=['In-Time', 'OOT'], \n",
        "                           vert=False,  # HORIZONTAL\n",
        "                           patch_artist=True,\n",
        "                           showmeans=False, \n",
        "                           showfliers=True)\n",
        "            \n",
        "            # Style boxes\n",
        "            colors = ['lightblue', 'lightcoral']\n",
        "            for patch, color in zip(bp['boxes'], colors):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "            \n",
        "            # Style outliers with LOW ALPHA for dense outliers\n",
        "            for flier in bp['fliers']:\n",
        "                flier.set_marker('o')\n",
        "                flier.set_markerfacecolor('black')\n",
        "                flier.set_markersize(3)\n",
        "                flier.set_alpha(0.1)  # Very low alpha\n",
        "            \n",
        "            ax.set_title(f'{feature}\\n({table_folder_name})', fontsize=12, fontweight='bold')\n",
        "            ax.set_xlabel('Value', fontsize=10)\n",
        "            ax.set_ylabel('Time Period', fontsize=10)\n",
        "            ax.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            \n",
        "            # Save\n",
        "            plot_file = f\"{table_plot_folder}{feature}.png\"\n",
        "            save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "            plt.close(fig)\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "print(\"✓ Setup complete with optimizations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/feature_profiling/\"\n",
        "PLOT_PATH = OUTPUT_PATH + \"plots/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "dbutils.fs.mkdirs(PLOT_PATH)\n",
        "\n",
        "SAMPLING_RATIO = 0.01\n",
        "PLOT_SAMPLING_RATIO = 0.01\n",
        "OOT_START_DATE = '2024-01-01'\n",
        "\n",
        "# Feature tables to analyze\n",
        "TABLES = [\n",
        "    (\"cust\", \"cust_basic_sumary\", ''),\n",
        "    (\"cust\", \"batch_credit_bureau\", ''),\n",
        "    (\"dem\", \"acct\", 2438),\n",
        "    (\"cc\", \"acct\", 2444),\n",
        "    (\"loc\", \"acct\", 2442),\n",
        "    (\"loan\", \"acct\", 2439),\n",
        "    (\"mtg\", \"acct\", 2440),\n",
        "    (\"inv\", \"acct\", 1331),\n",
        "    (\"dem\", \"acct_trans\", 2438),\n",
        "    (\"cc\", \"acct_trans\", 2444),\n",
        "]\n",
        "\n",
        "# Load metadata\n",
        "feature_metadata_rows = spark.read.text(f\"{DATA_PATH}/feature/feature_metadata.jsonl\").collect()\n",
        "feature_metadata = json.loads('\\n'.join([row.value for row in feature_metadata_rows]))\n",
        "\n",
        "print(\"✓ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Strategy: Sampled Full-Table (Accuracy Prioritized)\n",
        "\n",
        "### Why This Approach?\n",
        "This notebook calculates **median, percentiles (p1, p99)** which **CANNOT be calculated incrementally**. We must see all values to sort/rank them accurately.\n",
        "\n",
        "### Memory Efficiency:\n",
        "- **Memory usage**: Scales with SAMPLING_RATIO\n",
        "- **Mitigation**: Process one table at a time (10 tables total), free memory between tables\n",
        "- **Recommendation for memory issue**: Use `SAMPLING_RATIO = 0.01` (1%) for accurate results with manageable memory\n",
        "\n",
        "### How It Works:\n",
        "```\n",
        "For each table (10 total):\n",
        "  1. Load FULL table via Spark (efficient Parquet reading)\n",
        "  2. Apply sampling at Spark level: .sample(fraction=SAMPLING_RATIO)\n",
        "  3. Convert to pandas: .toPandas()\n",
        "  4. Calculate accurate statistics:\n",
        "     - median: df[col].median() ← requires sorted values\n",
        "     - p99: df[col].quantile(0.99) ← requires percentile calculation\n",
        "     - mean, min, max, n_unique, etc.\n",
        "  5. Free memory before next table (del df; gc.collect())\n",
        "```\n",
        "\n",
        "### Why Incremental Doesn't Work Here:\n",
        "- ❌ median(chunk1) + median(chunk2) ≠ median(all_data)\n",
        "- ❌ p99(chunk1) combined with p99(chunk2) ≠ p99(all_data)\n",
        "- ✅ Must see all sampled values together to calculate correct percentiles\n",
        "- ✅ 1% sampling gives exact statistics on representative sample\n",
        "\n",
        "### Alternative Considered:\n",
        "Could use approximate algorithms (T-Digest, Q-Digest) for streaming percentiles, but:\n",
        "- ❌ Introduces approximation error\n",
        "- ❌ Complex to implement and debug\n",
        "- ✅ 1% sampling gives exact results with manageable memory\n",
        "- ✅ Simpler code is easier to maintain\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Criteria and Expected Results\n",
        "\n",
        "### ✅ **Profiling Succeeds If**:\n",
        "- All tables processed successfully\n",
        "- Statistics calculated for **all features** (numerical + categorical) in metadata\n",
        "- **Separate statistics** calculated for In-Time vs OOT periods\n",
        "- No excessive missing values (>99.9%) unless expected\n",
        "- Reasonable value ranges (no extreme outliers unless business-valid)\n",
        "- Categorical features have reasonable cardinality\n",
        "- **Time-period comparison** shows expected differences between In-Time and OOT\n",
        "\n",
        "### 📊 **Statistics Calculated Per Feature (Per Time Period)**:\n",
        "| Statistic | Numerical | Categorical | Notes |\n",
        "|-----------|-----------|-------------|-------|\n",
        "| time_period | ✓ | ✓ | 'In-Time' or 'OOT' |\n",
        "| feature | ✓ | ✓ | Feature name |\n",
        "| data_type | ✓ | ✓ | Identifies feature type |\n",
        "| pct_zero | ✓ | ✓ | % of values that are 0 |\n",
        "| n_unique | ✓ | ✓ | Number of distinct values |\n",
        "| most_frequent_value | ✓ | ✓ | Mode |\n",
        "| pct_most_frequent | ✓ | ✓ | % of samples with mode |\n",
        "| min | ✓ | ✓ | Minimum value |\n",
        "| max | ✓ | ✓ | Maximum value |\n",
        "| p1 | ✓ | ✗ | 1st percentile |\n",
        "| median (p50) | ✓ | ✗ | 50th percentile |\n",
        "| p99 | ✓ | ✗ | 99th percentile |\n",
        "| mean | ✓ | ✗ | Average value |\n",
        "\n",
        "### 📈 **Time-Period Analysis**:\n",
        "Each feature has **two rows** in the output CSV:\n",
        "- **Row 1**: Statistics for 'In-Time' period (before 2024-01-01)\n",
        "- **Row 2**: Statistics for 'OOT' period (2024-01-01 and after)\n",
        "\n",
        "**What to Compare**:\n",
        "- **Distributions**: Compare median, mean, percentiles between periods\n",
        "- **Sparsity**: Compare pct_zero (may increase/decrease over time)\n",
        "- **Cardinality**: Compare n_unique for categorical features\n",
        "- **Ranges**: Compare min/max values (may indicate data quality issues)\n",
        "- **Mode**: Compare most_frequent_value (distribution shifts)\n",
        "\n",
        "### ⚠️ **Potential Issues to Flag**:\n",
        "- **Features with >99% zeros** (may be redundant or sparse)\n",
        "- **Features with only 1 unique value** (constant features - no information)\n",
        "- **Features with extreme ranges** (may need normalization or clipping)\n",
        "- **Categorical features with very high cardinality** (>1000 categories - may need bucketing)\n",
        "- **Features missing from certain tables** (expected for table-specific features)\n",
        "- **Large differences between In-Time and OOT**:\n",
        "  - Significant shifts in median/mean (potential drift)\n",
        "  - Large changes in pct_zero (sparsity changes)\n",
        "  - Cardinality changes in categoricals (new categories appear/disappear)\n",
        "\n",
        "### 📊 **Boxplot Visualizations**:\n",
        "- **One boxplot per numerical feature** comparing In-Time vs OOT\n",
        "- Saved individually: `plots/{table_name}/{feature_name}.png`\n",
        "- **What to look for**:\n",
        "  - Distribution shifts (boxes at different positions)\n",
        "  - Spread changes (box width differences)\n",
        "  - Outlier patterns (fliers in different locations)\n",
        "  - Median differences (horizontal line position)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE PROFILING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        continue\n",
        "    \n",
        "    num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "    cat_features = list(feature_metadata[fam_name][table_meta_key].get(\"cat_features\", {}).keys())\n",
        "    \n",
        "    # OPTIMIZATION: Load table once, process features in batches to avoid driver memory limit\n",
        "    df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "    if SAMPLING_RATIO < 1.0:\n",
        "        df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    \n",
        "    # IMPORTANT FIX: Process features in batches to avoid exceeding driver result size limit\n",
        "    # Instead of loading all features at once (which can exceed 126 GB limit),\n",
        "    # we load features in small batches\n",
        "    \n",
        "    BATCH_SIZE = 4  # Process 4 features at a time to avoid driver memory limit\n",
        "    all_features = num_features + cat_features\n",
        "    all_stats = []\n",
        "    \n",
        "    # Always include efectv_dt if available\n",
        "    has_efectv_dt = 'efectv_dt' in df_spark.columns\n",
        "    \n",
        "    # Process features in batches\n",
        "    for batch_idx in range(0, len(all_features), BATCH_SIZE):\n",
        "        batch_features = all_features[batch_idx:batch_idx + BATCH_SIZE]\n",
        "        batch_features_in_table = [f for f in batch_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(batch_features_in_table) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Load only this batch of features\n",
        "        cols_batch = batch_features_in_table\n",
        "        if has_efectv_dt:\n",
        "            cols_batch = ['efectv_dt'] + cols_batch\n",
        "        \n",
        "        df_batch = df_spark.select(cols_batch).toPandas()\n",
        "        \n",
        "        # Split by time period if efectv_dt is available\n",
        "        if has_efectv_dt:\n",
        "            df_batch['efectv_dt'] = pd.to_datetime(df_batch['efectv_dt'])\n",
        "            oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "            df_batch['time_period'] = np.where(df_batch['efectv_dt'] >= oot_date, 'OOT', 'In-Time')\n",
        "            \n",
        "            # Profile this batch for both time periods\n",
        "            for period_name in ['In-Time', 'OOT']:\n",
        "                period_df = df_batch[df_batch['time_period'] == period_name]\n",
        "                \n",
        "                if len(period_df) > 0:\n",
        "                    for feature in batch_features_in_table:\n",
        "                        if feature not in period_df.columns:\n",
        "                            continue\n",
        "                        is_cat = feature in cat_features\n",
        "                        \n",
        "                        # Use optimized profiling function\n",
        "                        feature_stats = profile_feature_optimized(feature, period_df, is_cat)\n",
        "                        feature_stats['time_period'] = period_name\n",
        "                        all_stats.append(feature_stats)\n",
        "        else:\n",
        "            # No time splitting - profile all data for this batch\n",
        "            for feature in batch_features_in_table:\n",
        "                if feature not in df_batch.columns:\n",
        "                    continue\n",
        "                is_cat = feature in cat_features\n",
        "                \n",
        "                feature_stats = profile_feature_optimized(feature, df_batch, is_cat)\n",
        "                feature_stats['time_period'] = 'All'\n",
        "                all_stats.append(feature_stats)\n",
        "        \n",
        "        # Free memory after each batch\n",
        "        del df_batch\n",
        "        gc.collect()\n",
        "        \n",
        "        # Reduce print frequency to avoid I/O stream timeout\n",
        "        if batch_idx > 0 and (batch_idx % 50 == 0):\n",
        "            print(f\"    Processed {min(batch_idx, len(all_features))}/{len(all_features)} features...\")\n",
        "    \n",
        "    # Save profiling results\n",
        "    if all_stats:\n",
        "        results_df = pd.DataFrame(all_stats)\n",
        "        col_order = ['time_period', 'feature', 'data_type'] + [c for c in results_df.columns if c not in ['time_period', 'feature', 'data_type']]\n",
        "        results_df = results_df[col_order]\n",
        "        save_pandas_to_csv_adls(results_df, f\"{OUTPUT_PATH}feature_profile_{fam_name}_{table}.csv\")\n",
        "    \n",
        "    # Create boxplots (need to reload data with time_period for plotting)\n",
        "    if has_efectv_dt:\n",
        "        # Load only numerical features needed for plotting in batches\n",
        "        plot_features = [f for f in num_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(plot_features) > 0:\n",
        "            print(f\"  Creating {len(plot_features)} individual boxplots (parallel processing)...\")\n",
        "            \n",
        "            table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "            table_plot_folder = f\"{PLOT_PATH}{table_folder_name}/\"\n",
        "            dbutils.fs.mkdirs(table_plot_folder)\n",
        "            \n",
        "            saved_count = 0\n",
        "            failed_count = 0\n",
        "            \n",
        "            # Process plotting in feature batches to avoid memory AND I/O stream issues\n",
        "            PLOT_BATCH_SIZE = 4  # Smaller batches to avoid overwhelming I/O\n",
        "            \n",
        "            for plot_batch_idx in range(0, len(plot_features), PLOT_BATCH_SIZE):\n",
        "                plot_batch = plot_features[plot_batch_idx:plot_batch_idx + PLOT_BATCH_SIZE]\n",
        "                \n",
        "                # Load only this batch for plotting\n",
        "                df_plot = df_spark.select(['efectv_dt'] + plot_batch).toPandas()\n",
        "                df_plot['efectv_dt'] = pd.to_datetime(df_plot['efectv_dt'])\n",
        "                oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "                df_plot['time_period'] = np.where(df_plot['efectv_dt'] >= oot_date, 'OOT', 'In-Time')\n",
        "                \n",
        "                # Pre-filter masks\n",
        "                intime_mask = df_plot['time_period'] == 'In-Time'\n",
        "                oot_mask = df_plot['time_period'] == 'OOT'\n",
        "                \n",
        "                # OPTIMIZATION: Reduce parallel jobs to avoid I/O stream timeout\n",
        "                # Use n_jobs=2 instead of 4, and verbose=0 to suppress output\n",
        "                results = Parallel(n_jobs=2, backend='threading', verbose=0)(\n",
        "                    delayed(create_boxplot_optimized)(\n",
        "                        feature,\n",
        "                        df_plot.loc[intime_mask, feature],\n",
        "                        df_plot.loc[oot_mask, feature],\n",
        "                        table_folder_name,\n",
        "                        table_plot_folder\n",
        "                    )\n",
        "                    for feature in plot_batch\n",
        "                )\n",
        "                \n",
        "                saved_count += sum(results)\n",
        "                failed_count += len(results) - sum(results)\n",
        "                \n",
        "                del df_plot\n",
        "                gc.collect()\n",
        "                \n",
        "                # Print progress less frequently to avoid I/O congestion\n",
        "                if plot_batch_idx > 0 and (plot_batch_idx % 25 == 0):\n",
        "                    print(f\"      Boxplots: {saved_count + failed_count}/{len(plot_features)} processed...\")\n",
        "            \n",
        "            print(f\"  ✓ Boxplots saved: {saved_count} successful, {failed_count} failed\")\n",
        "            print(f\"    Location: {table_plot_folder}\")\n",
        "    \n",
        "    del df_spark\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n✓ Feature profiling complete\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
