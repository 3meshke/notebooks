{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Profiling by Table (Pandas Version)\n",
        "\n",
        "## Overview\n",
        "Comprehensive feature profiling using **pandas** with memory-efficient chunk processing.\n",
        "\n",
        "### Pandas Optimizations:\n",
        "- **Chunked reading**: Process data in manageable chunks\n",
        "- **Streaming statistics**: Calculate stats without loading full table\n",
        "- **Memory efficient**: Use pandas iterators and explicit cleanup\n",
        "\n",
        "### Statistics Calculated:\n",
        "- Data type, % zeros, n_unique\n",
        "- Most frequent value and percentage\n",
        "- Percentiles: min, 1%, 50%, 99%, max, mean\n",
        "\n",
        "### Outputs:\n",
        "- Feature profiling CSVs per table with **separate statistics for In-Time vs OOT**\n",
        "  - Each feature has two rows: one for 'In-Time' period, one for 'OOT' period\n",
        "  - Includes `time_period` column to distinguish periods\n",
        "- Individual boxplots for each feature (one PNG per feature)\n",
        "  - Saved in table-specific folders: `plots/{table_name}/`\n",
        "  - Each file named: `{feature_name}.png`\n",
        "  - Comparing OOT vs in-time distributions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pandas==2 -i https://repo.td.com/repository/pypi-all/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "print(pandas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import gc\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Helper functions\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"âœ“ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    import tempfile, os\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    # Removed print to reduce I/O congestion\n",
        "\n",
        "# Spark-based profiling function (MUCH faster and memory-efficient)\n",
        "def profile_features_spark(df_spark, features, num_features, cat_features, time_period=None):\n",
        "    \"\"\"Profile features using Spark - avoids memory issues\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for feature in features:\n",
        "        if feature not in df_spark.columns:\n",
        "            continue\n",
        "            \n",
        "        is_cat = feature in cat_features\n",
        "        \n",
        "        # Base stats using Spark\n",
        "        df_feat = df_spark.select(feature).filter(F.col(feature).isNotNull())\n",
        "        total_count = df_spark.count()\n",
        "        non_null_count = df_feat.count()\n",
        "        \n",
        "        if non_null_count == 0:\n",
        "            results.append({\n",
        "                'feature': feature,\n",
        "                'data_type': 'categorical' if is_cat else 'numerical',\n",
        "                'time_period': time_period or 'All',\n",
        "                'pct_zero': None, 'n_unique': 0, 'most_frequent_value': None,\n",
        "                'pct_most_frequent': None, 'min': None, 'max': None,\n",
        "                'p1': None, 'median': None, 'p99': None, 'mean': None\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        # Calculate basic stats in Spark\n",
        "        stats = {\n",
        "            'feature': feature,\n",
        "            'data_type': 'categorical' if is_cat else 'numerical',\n",
        "            'time_period': time_period or 'All'\n",
        "        }\n",
        "        \n",
        "        # Percent zeros (for numerical)\n",
        "        if not is_cat:\n",
        "            zero_count = df_spark.filter(F.col(feature) == 0).count()\n",
        "            stats['pct_zero'] = zero_count / total_count\n",
        "        else:\n",
        "            stats['pct_zero'] = None\n",
        "        \n",
        "        # Number of unique values\n",
        "        stats['n_unique'] = df_feat.select(feature).distinct().count()\n",
        "        \n",
        "        # Most frequent value\n",
        "        mode_df = df_feat.groupBy(feature).count().orderBy(F.desc('count')).limit(1).collect()\n",
        "        if mode_df:\n",
        "            stats['most_frequent_value'] = mode_df[0][0]\n",
        "            stats['pct_most_frequent'] = mode_df[0][1] / non_null_count\n",
        "        else:\n",
        "            stats['most_frequent_value'] = None\n",
        "            stats['pct_most_frequent'] = None\n",
        "        \n",
        "        if not is_cat:\n",
        "            # For numerical features, use Spark's built-in functions\n",
        "            try:\n",
        "                # Cast to double for numerical operations\n",
        "                df_numeric = df_feat.select(F.col(feature).cast('double').alias(feature))\n",
        "                \n",
        "                # Use Spark's summary for basic stats (VERY fast)\n",
        "                summary_stats = df_numeric.select(\n",
        "                    F.min(feature).alias('min'),\n",
        "                    F.max(feature).alias('max'),\n",
        "                    F.mean(feature).alias('mean')\n",
        "                ).collect()[0]\n",
        "                \n",
        "                stats['min'] = float(summary_stats['min']) if summary_stats['min'] is not None else None\n",
        "                stats['max'] = float(summary_stats['max']) if summary_stats['max'] is not None else None\n",
        "                stats['mean'] = float(summary_stats['mean']) if summary_stats['mean'] is not None else None\n",
        "                \n",
        "                # Use approxQuantile for percentiles (much faster than exact)\n",
        "                percentiles = df_numeric.approxQuantile(feature, [0.01, 0.5, 0.99], 0.01)\n",
        "                stats['p1'] = float(percentiles[0]) if percentiles[0] is not None else None\n",
        "                stats['median'] = float(percentiles[1]) if percentiles[1] is not None else None\n",
        "                stats['p99'] = float(percentiles[2]) if percentiles[2] is not None else None\n",
        "                \n",
        "            except:\n",
        "                stats.update({'min': None, 'p1': None, 'median': None, 'p99': None, 'max': None, 'mean': None})\n",
        "        else:\n",
        "            # For categorical, get min/max\n",
        "            try:\n",
        "                min_max = df_feat.select(F.min(feature).alias('min'), F.max(feature).alias('max')).collect()[0]\n",
        "                stats['min'] = min_max['min']\n",
        "                stats['max'] = min_max['max']\n",
        "                stats.update({'p1': None, 'median': None, 'p99': None, 'mean': None})\n",
        "            except:\n",
        "                stats.update({'min': None, 'max': None, 'p1': None, 'median': None, 'p99': None, 'mean': None})\n",
        "        \n",
        "        results.append(stats)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Batch boxplot creation using Spark sampling\n",
        "def create_boxplots_batch_spark(df_spark, features, table_folder_name, table_plot_folder, oot_date):\n",
        "    \"\"\"Create boxplots using Spark sampling - much more memory efficient\"\"\"\n",
        "    saved_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    # Sample for plotting (much smaller dataset)\n",
        "    PLOT_SAMPLE_SIZE = 10000  # Fixed sample size for consistency\n",
        "    \n",
        "    for feature in features:\n",
        "        try:\n",
        "            # Get sample data from Spark\n",
        "            df_sample = df_spark.select('efectv_dt', feature).filter(\n",
        "                F.col(feature).isNotNull()\n",
        "            ).limit(PLOT_SAMPLE_SIZE).toPandas()\n",
        "            \n",
        "            if len(df_sample) == 0:\n",
        "                failed_count += 1\n",
        "                continue\n",
        "            \n",
        "            # Convert to numeric\n",
        "            df_sample[feature] = pd.to_numeric(df_sample[feature], errors='coerce')\n",
        "            df_sample = df_sample.dropna()\n",
        "            \n",
        "            if len(df_sample) == 0:\n",
        "                failed_count += 1\n",
        "                continue\n",
        "            \n",
        "            # Split by time period\n",
        "            df_sample['efectv_dt'] = pd.to_datetime(df_sample['efectv_dt'])\n",
        "            intime_data = df_sample[df_sample['efectv_dt'] < oot_date][feature].values\n",
        "            oot_data = df_sample[df_sample['efectv_dt'] >= oot_date][feature].values\n",
        "            \n",
        "            if len(intime_data) > 0 and len(oot_data) > 0:\n",
        "                # Create plot\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                \n",
        "                bp = ax.boxplot([intime_data, oot_data], \n",
        "                               labels=['In-Time', 'OOT'], \n",
        "                               vert=False,\n",
        "                               patch_artist=True,\n",
        "                               showmeans=False,\n",
        "                               showfliers=True)\n",
        "                \n",
        "                # Style\n",
        "                colors = ['lightblue', 'lightcoral']\n",
        "                for patch, color in zip(bp['boxes'], colors):\n",
        "                    patch.set_facecolor(color)\n",
        "                    patch.set_alpha(0.7)\n",
        "                \n",
        "                for flier in bp['fliers']:\n",
        "                    flier.set_marker('o')\n",
        "                    flier.set_markerfacecolor('black')\n",
        "                    flier.set_markersize(3)\n",
        "                    flier.set_alpha(0.1)\n",
        "                \n",
        "                ax.set_title(f'{feature}\\n({table_folder_name})', fontsize=12, fontweight='bold')\n",
        "                ax.set_xlabel('Value', fontsize=10)\n",
        "                ax.set_ylabel('Time Period', fontsize=10)\n",
        "                ax.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                \n",
        "                # Save\n",
        "                plot_file = f\"{table_plot_folder}{feature}.png\"\n",
        "                save_plot_to_adls(fig, plot_file, dpi=150)\n",
        "                plt.close(fig)\n",
        "                \n",
        "                saved_count += 1\n",
        "            else:\n",
        "                failed_count += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            continue\n",
        "    \n",
        "    return saved_count, failed_count\n",
        "\n",
        "print(\"âœ“ Setup complete with Spark optimizations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/feature_profiling/\"\n",
        "PLOT_PATH = OUTPUT_PATH + \"plots/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "dbutils.fs.mkdirs(PLOT_PATH)\n",
        "\n",
        "SAMPLING_RATIO = 0.01\n",
        "PLOT_SAMPLING_RATIO = 0.01\n",
        "OOT_START_DATE = '2024-01-01'\n",
        "\n",
        "# Feature tables to analyze\n",
        "TABLES = [\n",
        "    (\"cust\", \"batch_credit_bureau\", ''),\n",
        "    (\"cust\", \"cust_basic_sumary\", ''),\n",
        "    (\"dem\", \"acct_trans\", 2438),\n",
        "    (\"cc\", \"acct_trans\", 2444),\n",
        "    (\"dem\", \"acct\", 2438),\n",
        "    (\"cc\", \"acct\", 2444),\n",
        "    (\"loc\", \"acct\", 2442),\n",
        "    (\"loan\", \"acct\", 2439),\n",
        "    (\"mtg\", \"acct\", 2440),\n",
        "    (\"inv\", \"acct\", 1331),\n",
        "]\n",
        "\n",
        "# Load metadata\n",
        "feature_metadata_rows = spark.read.text(f\"{DATA_PATH}/feature/feature_metadata.jsonl\").collect()\n",
        "feature_metadata = json.loads('\\n'.join([row.value for row in feature_metadata_rows]))\n",
        "\n",
        "print(\"âœ“ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Strategy: Sampled Full-Table (Accuracy Prioritized)\n",
        "\n",
        "### Why This Approach?\n",
        "This notebook calculates **median, percentiles (p1, p99)** which **CANNOT be calculated incrementally**. We must see all values to sort/rank them accurately.\n",
        "\n",
        "### Memory Efficiency:\n",
        "- **Memory usage**: Scales with SAMPLING_RATIO\n",
        "- **Mitigation**: Process one table at a time (10 tables total), free memory between tables\n",
        "- **Recommendation for memory issue**: Use `SAMPLING_RATIO = 0.01` (1%) for accurate results with manageable memory\n",
        "\n",
        "### How It Works:\n",
        "```\n",
        "For each table (10 total):\n",
        "  1. Load FULL table via Spark (efficient Parquet reading)\n",
        "  2. Apply sampling at Spark level: .sample(fraction=SAMPLING_RATIO)\n",
        "  3. Convert to pandas: .toPandas()\n",
        "  4. Calculate accurate statistics:\n",
        "     - median: df[col].median() â† requires sorted values\n",
        "     - p99: df[col].quantile(0.99) â† requires percentile calculation\n",
        "     - mean, min, max, n_unique, etc.\n",
        "  5. Free memory before next table (del df; gc.collect())\n",
        "```\n",
        "\n",
        "### Why Incremental Doesn't Work Here:\n",
        "- âŒ median(chunk1) + median(chunk2) â‰  median(all_data)\n",
        "- âŒ p99(chunk1) combined with p99(chunk2) â‰  p99(all_data)\n",
        "- âœ… Must see all sampled values together to calculate correct percentiles\n",
        "- âœ… 1% sampling gives exact statistics on representative sample\n",
        "\n",
        "### Alternative Considered:\n",
        "Could use approximate algorithms (T-Digest, Q-Digest) for streaming percentiles, but:\n",
        "- âŒ Introduces approximation error\n",
        "- âŒ Complex to implement and debug\n",
        "- âœ… 1% sampling gives exact results with manageable memory\n",
        "- âœ… Simpler code is easier to maintain\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Criteria and Expected Results\n",
        "\n",
        "### âœ… **Profiling Succeeds If**:\n",
        "- All tables processed successfully\n",
        "- Statistics calculated for **all features** (numerical + categorical) in metadata\n",
        "- **Separate statistics** calculated for In-Time vs OOT periods\n",
        "- No excessive missing values (>99.9%) unless expected\n",
        "- Reasonable value ranges (no extreme outliers unless business-valid)\n",
        "- Categorical features have reasonable cardinality\n",
        "- **Time-period comparison** shows expected differences between In-Time and OOT\n",
        "\n",
        "### ðŸ“Š **Statistics Calculated Per Feature (Per Time Period)**:\n",
        "| Statistic | Numerical | Categorical | Notes |\n",
        "|-----------|-----------|-------------|-------|\n",
        "| time_period | âœ“ | âœ“ | 'In-Time' or 'OOT' |\n",
        "| feature | âœ“ | âœ“ | Feature name |\n",
        "| data_type | âœ“ | âœ“ | Identifies feature type |\n",
        "| pct_zero | âœ“ | âœ“ | % of values that are 0 |\n",
        "| n_unique | âœ“ | âœ“ | Number of distinct values |\n",
        "| most_frequent_value | âœ“ | âœ“ | Mode |\n",
        "| pct_most_frequent | âœ“ | âœ“ | % of samples with mode |\n",
        "| min | âœ“ | âœ“ | Minimum value |\n",
        "| max | âœ“ | âœ“ | Maximum value |\n",
        "| p1 | âœ“ | âœ— | 1st percentile |\n",
        "| median (p50) | âœ“ | âœ— | 50th percentile |\n",
        "| p99 | âœ“ | âœ— | 99th percentile |\n",
        "| mean | âœ“ | âœ— | Average value |\n",
        "\n",
        "### ðŸ“ˆ **Time-Period Analysis**:\n",
        "Each feature has **two rows** in the output CSV:\n",
        "- **Row 1**: Statistics for 'In-Time' period (before 2024-01-01)\n",
        "- **Row 2**: Statistics for 'OOT' period (2024-01-01 and after)\n",
        "\n",
        "**What to Compare**:\n",
        "- **Distributions**: Compare median, mean, percentiles between periods\n",
        "- **Sparsity**: Compare pct_zero (may increase/decrease over time)\n",
        "- **Cardinality**: Compare n_unique for categorical features\n",
        "- **Ranges**: Compare min/max values (may indicate data quality issues)\n",
        "- **Mode**: Compare most_frequent_value (distribution shifts)\n",
        "\n",
        "### âš ï¸ **Potential Issues to Flag**:\n",
        "- **Features with >99% zeros** (may be redundant or sparse)\n",
        "- **Features with only 1 unique value** (constant features - no information)\n",
        "- **Features with extreme ranges** (may need normalization or clipping)\n",
        "- **Categorical features with very high cardinality** (>1000 categories - may need bucketing)\n",
        "- **Features missing from certain tables** (expected for table-specific features)\n",
        "- **Large differences between In-Time and OOT**:\n",
        "  - Significant shifts in median/mean (potential drift)\n",
        "  - Large changes in pct_zero (sparsity changes)\n",
        "  - Cardinality changes in categoricals (new categories appear/disappear)\n",
        "\n",
        "### ðŸ“Š **Boxplot Visualizations**:\n",
        "- **One boxplot per numerical feature** comparing In-Time vs OOT\n",
        "- Saved individually: `plots/{table_name}/{feature_name}.png`\n",
        "- **What to look for**:\n",
        "  - Distribution shifts (boxes at different positions)\n",
        "  - Spread changes (box width differences)\n",
        "  - Outlier patterns (fliers in different locations)\n",
        "  - Median differences (horizontal line position)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE PROFILING (SPARK-OPTIMIZED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for fam_name, table, fam in TABLES:\n",
        "    print(f\"\\nProcessing: {fam_name}-{table}\")\n",
        "    \n",
        "    table_path = f\"{DATA_PATH}/feature/{table}/parquet\" if not fam else f\"{DATA_PATH}/feature/{table}_{fam}/parquet\"\n",
        "    table_meta_key = table if not fam else f\"{table}_{fam}\"\n",
        "    \n",
        "    if fam_name not in feature_metadata or table_meta_key not in feature_metadata[fam_name]:\n",
        "        continue\n",
        "    \n",
        "    num_features = feature_metadata[fam_name][table_meta_key].get(\"num_features\", [])\n",
        "    cat_features = list(feature_metadata[fam_name][table_meta_key].get(\"cat_features\", {}).keys())\n",
        "    \n",
        "    # OPTIMIZATION: Load table once, process features in batches to avoid driver memory limit\n",
        "    df_spark = spark.read.format(\"parquet\").load(table_path)\n",
        "    if SAMPLING_RATIO < 1.0:\n",
        "        df_spark = df_spark.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    \n",
        "    # SPARK OPTIMIZATION: Process all features using Spark native functions\n",
        "    # Cache DataFrame for reuse\n",
        "    df_spark.cache()\n",
        "    \n",
        "    all_features = num_features + cat_features\n",
        "    all_stats = []\n",
        "    has_efectv_dt = 'efectv_dt' in df_spark.columns\n",
        "    \n",
        "    if has_efectv_dt:\n",
        "        # Split into time periods using Spark (much more efficient)\n",
        "        oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "        \n",
        "        # Create In-Time DataFrame\n",
        "        df_intime = df_spark.filter(\n",
        "            F.col('efectv_dt') < F.lit(oot_date)\n",
        "        )\n",
        "        \n",
        "        # Create OOT DataFrame  \n",
        "        df_oot = df_spark.filter(\n",
        "            F.col('efectv_dt') >= F.lit(oot_date)\n",
        "        )\n",
        "        \n",
        "        # Profile In-Time features using Spark\n",
        "        print(\"  Profiling In-Time features using Spark...\")\n",
        "        intime_stats = profile_features_spark(df_intime, all_features, num_features, cat_features, 'In-Time')\n",
        "        all_stats.extend(intime_stats)\n",
        "        \n",
        "        # Profile OOT features using Spark\n",
        "        print(\"  Profiling OOT features using Spark...\")\n",
        "        oot_stats = profile_features_spark(df_oot, all_features, num_features, cat_features, 'OOT')\n",
        "        all_stats.extend(oot_stats)\n",
        "    else:\n",
        "        # No time splitting - profile all\n",
        "        print(\"  Profiling all features using Spark...\")\n",
        "        stats = profile_features_spark(df_spark, all_features, num_features, cat_features, 'All')\n",
        "        all_stats.extend(stats)\n",
        "    \n",
        "    # Save profiling results\n",
        "    if all_stats:\n",
        "        results_df = pd.DataFrame(all_stats)\n",
        "        col_order = ['time_period', 'feature', 'data_type'] + [c for c in results_df.columns if c not in ['time_period', 'feature', 'data_type']]\n",
        "        results_df = results_df[col_order]\n",
        "        save_pandas_to_csv_adls(results_df, f\"{OUTPUT_PATH}feature_profile_{fam_name}_{table}.csv\")\n",
        "    \n",
        "    # Create boxplots (need to reload data with time_period for plotting)\n",
        "    if has_efectv_dt:\n",
        "        # Load only numerical features needed for plotting in batches\n",
        "        plot_features = [f for f in num_features if f in df_spark.columns]\n",
        "        \n",
        "        if len(plot_features) > 0:\n",
        "            print(f\"  Creating {len(plot_features)} individual boxplots (parallel processing)...\")\n",
        "            \n",
        "            table_folder_name = f\"{fam_name}_{table}\" if not fam else f\"{fam_name}_{table}_{fam}\"\n",
        "            table_plot_folder = f\"{PLOT_PATH}{table_folder_name}/\"\n",
        "            dbutils.fs.mkdirs(table_plot_folder)\n",
        "            \n",
        "            # SPARK OPTIMIZATION: Create boxplots using Spark sampling\n",
        "            oot_date = pd.to_datetime(OOT_START_DATE)\n",
        "            saved_count, failed_count = create_boxplots_batch_spark(\n",
        "                df_spark, plot_features, table_folder_name, table_plot_folder, oot_date\n",
        "            )\n",
        "            \n",
        "            print(f\"  âœ“ Boxplots saved: {saved_count} successful, {failed_count} failed\")\n",
        "            print(f\"    Location: {table_plot_folder}\")\n",
        "    \n",
        "    # Unpersist cached DataFrame to free memory\n",
        "    df_spark.unpersist()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nâœ“ Feature profiling complete\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
