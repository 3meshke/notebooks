{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Integrity & Customer Uniqueness Validation\n",
        "\n",
        "## Overview\n",
        "This notebook validates the fundamental data integrity constraint: **customer-level isolation across chunks**.\n",
        "\n",
        "### Key Validation Rules:\n",
        "1. **Within each chunk**: A customer_id should appear multiple times (one per effective_date)\n",
        "2. **Across chunks**: A customer_id should appear in EXACTLY ONE chunk (all their samples in one chunk)\n",
        "3. **Chunk 0-255**: Train customers (in-time data before 2024)\n",
        "4. **Chunk 256-319**: Validation customers (in-time data before 2024)\n",
        "5. **OOT data**: 2024 effective dates present across all chunks\n",
        "\n",
        "### Outputs:\n",
        "- Customer count per chunk\n",
        "- Sample count per chunk by time period\n",
        "- Cross-chunk customer_id duplication report (should be ZERO)\n",
        "- Split confirmation (train/valid/OOT)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO, StringIO\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions for ADLS File Operations\n",
        "\n",
        "Functions to save CSV and PNG files directly to ADLS paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions to save files directly to ADLS rather than DBFS\n",
        "def save_pandas_to_csv_adls(df_pandas, adls_path):\n",
        "    \"\"\"Save pandas DataFrame to ADLS as CSV using dbutils\"\"\"\n",
        "    # Convert to CSV string\n",
        "    csv_string = df_pandas.to_csv(index=False)\n",
        "    # Write to ADLS using dbutils\n",
        "    dbutils.fs.put(adls_path, csv_string, overwrite=True)\n",
        "    print(f\"✓ Saved CSV to {adls_path}\")\n",
        "\n",
        "def save_plot_to_adls(fig, adls_path, dpi=150):\n",
        "    \"\"\"Save matplotlib figure to ADLS as PNG using dbutils\"\"\"\n",
        "    # Save to bytes buffer\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    # Convert to base64 for dbutils (or save to temp local then copy)\n",
        "    # Alternative: save locally to DBFS first, then copy to ADLS\n",
        "    import tempfile\n",
        "    import os\n",
        "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.png', delete=False) as tmp:\n",
        "        tmp.write(buf.getvalue())\n",
        "        tmp_path = tmp.name\n",
        "    \n",
        "    # Copy from local temp to ADLS\n",
        "    dbutils.fs.cp(f\"file:{tmp_path}\", adls_path)\n",
        "    os.remove(tmp_path)\n",
        "    print(f\"✓ Saved plot to {adls_path}\")\n",
        "\n",
        "print(\"✓ ADLS helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration & Paths\n",
        "\n",
        "Define data paths and key parameters for the validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CONFIGURATION ====================\n",
        "\n",
        "# Data paths on ADLS\n",
        "DATA_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/data/\"\n",
        "\n",
        "# Target paths (contains customer_id and effective_date)\n",
        "TARGET_TRAIN_VAL_PATH = DATA_PATH + \"target/cust/all_products_chunk_320/train_val/\"\n",
        "TARGET_TEST_PATH = DATA_PATH + \"target/cust/all_products_chunk_320/test/\"\n",
        "\n",
        "# Output paths for results\n",
        "OUTPUT_PATH = \"abfss://home@edaaaazepcalayelaye0001.dfs.core.windows.net/MD_Artifacts/money-out/mv/eda_validation/data_integrity/\"\n",
        "dbutils.fs.mkdirs(OUTPUT_PATH)\n",
        "\n",
        "# Chunk configuration\n",
        "TOTAL_CHUNKS = 320\n",
        "TRAIN_CHUNKS = list(range(0, 256))  # 256 chunks: 0-255\n",
        "VALID_CHUNKS = list(range(256, 320))  # 64 chunks: 256-319\n",
        "ALL_CHUNKS = list(range(0, 320))\n",
        "\n",
        "# Time split configuration\n",
        "OOT_START_DATE = '2024-01-01'  # Out-of-time starts from 2024\n",
        "\n",
        "# Sampling configuration (for quick testing)\n",
        "SAMPLING_RATIO = 1.0  # 1.0 = 100% (use all data), 0.01 = 1% sample\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  - Total chunks: {TOTAL_CHUNKS}\")\n",
        "print(f\"  - Train chunks: {len(TRAIN_CHUNKS)} (0-255)\")\n",
        "print(f\"  - Valid chunks: {len(VALID_CHUNKS)} (256-319)\")\n",
        "print(f\"  - OOT start date: {OOT_START_DATE}\")\n",
        "print(f\"  - Sampling ratio: {SAMPLING_RATIO*100}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Target Data\n",
        "\n",
        "Load train_val and test target data to extract customer_id and effective_date information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train_val data (in-time: train + validation customers)\n",
        "# CSV reader doesn't auto-discover partitions like Parquet, so we load all chunks and add chunk column manually\n",
        "print(\"Loading train_val data from all chunks...\")\n",
        "\n",
        "# Load all chunks by iterating through chunk directories\n",
        "df_train_val = None\n",
        "for chunk_id in range(TOTAL_CHUNKS):\n",
        "    chunk_path = f\"{TARGET_TRAIN_VAL_PATH}chunk={chunk_id}/\"\n",
        "    try:\n",
        "        df_chunk = spark.read.option(\"delimiter\", \",\") \\\n",
        "            .option(\"quoteMode\", \"NONE\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"escape\", \"\\\\\") \\\n",
        "            .csv(chunk_path)\n",
        "        \n",
        "        # Add chunk column\n",
        "        df_chunk = df_chunk.withColumn(\"chunk\", F.lit(chunk_id))\n",
        "        \n",
        "        # Union with existing data\n",
        "        if df_train_val is None:\n",
        "            df_train_val = df_chunk\n",
        "        else:\n",
        "            df_train_val = df_train_val.union(df_chunk)\n",
        "    except:\n",
        "        pass  # Skip if chunk doesn't exist\n",
        "\n",
        "print(f\"  Loaded {TOTAL_CHUNKS} chunks for train_val\")\n",
        "\n",
        "# Load test data (OOT: 2024 data)\n",
        "print(\"Loading test data from all chunks...\")\n",
        "df_test = None\n",
        "for chunk_id in range(TOTAL_CHUNKS):\n",
        "    chunk_path = f\"{TARGET_TEST_PATH}chunk={chunk_id}/\"\n",
        "    try:\n",
        "        df_chunk = spark.read.option(\"delimiter\", \",\") \\\n",
        "            .option(\"quoteMode\", \"NONE\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"escape\", \"\\\\\") \\\n",
        "            .csv(chunk_path)\n",
        "        \n",
        "        # Add chunk column\n",
        "        df_chunk = df_chunk.withColumn(\"chunk\", F.lit(chunk_id))\n",
        "        \n",
        "        # Union with existing data\n",
        "        if df_test is None:\n",
        "            df_test = df_chunk\n",
        "        else:\n",
        "            df_test = df_test.union(df_chunk)\n",
        "    except:\n",
        "        pass  # Skip if chunk doesn't exist\n",
        "\n",
        "print(f\"  Loaded {TOTAL_CHUNKS} chunks for test\")\n",
        "\n",
        "# Rename columns for consistency\n",
        "df_train_val = df_train_val.withColumnRenamed(\"pid\", \"cust_id\") \\\n",
        "    .withColumnRenamed(\"pred_dt\", \"efectv_dt\") \\\n",
        "    .withColumn(\"data_split\", F.lit(\"in-time\"))\n",
        "\n",
        "df_test = df_test.withColumnRenamed(\"pid\", \"cust_id\") \\\n",
        "    .withColumnRenamed(\"pred_dt\", \"efectv_dt\") \\\n",
        "    .withColumn(\"data_split\", F.lit(\"OOT\"))\n",
        "\n",
        "# Select only necessary columns and cache\n",
        "cols_needed = [\"cust_id\", \"efectv_dt\", \"chunk\", \"data_split\"]\n",
        "df_train_val = df_train_val.select(cols_needed)\n",
        "df_test = df_test.select(cols_needed)\n",
        "\n",
        "# Apply sampling if needed\n",
        "if SAMPLING_RATIO < 1.0:\n",
        "    print(f\"Applying {SAMPLING_RATIO*100}% sampling...\")\n",
        "    df_train_val = df_train_val.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "    df_test = df_test.sample(fraction=SAMPLING_RATIO, withReplacement=False, seed=42)\n",
        "\n",
        "# Union all data\n",
        "df_all = df_train_val.union(df_test).cache()\n",
        "\n",
        "print(f\"✓ Data loaded successfully\")\n",
        "print(f\"  - Total rows: {df_all.count():,}\")\n",
        "print(f\"  - Columns: {df_all.columns}\")\n",
        "df_all.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chunk-Level Summary Statistics\n",
        "\n",
        "Calculate basic statistics for each chunk to understand data distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics per chunk\n",
        "print(\"Calculating chunk-level statistics...\")\n",
        "\n",
        "chunk_stats = df_all.groupBy(\"chunk\").agg(\n",
        "    F.countDistinct(\"cust_id\").alias(\"unique_customers\"),\n",
        "    F.count(\"*\").alias(\"total_samples\"),\n",
        "    F.min(\"efectv_dt\").alias(\"min_date\"),\n",
        "    F.max(\"efectv_dt\").alias(\"max_date\")\n",
        ").orderBy(\"chunk\")\n",
        "\n",
        "# Convert to pandas for display and save\n",
        "chunk_stats_pd = chunk_stats.toPandas()\n",
        "chunk_stats_pd['chunk'] = chunk_stats_pd['chunk'].astype(int)\n",
        "chunk_stats_pd['chunk_type'] = chunk_stats_pd['chunk'].apply(\n",
        "    lambda x: 'train_OOT' if x < 256 else 'valid_OOT'\n",
        ")\n",
        "\n",
        "# Save to CSV on ADLS\n",
        "save_pandas_to_csv_adls(chunk_stats_pd, OUTPUT_PATH + \"chunk_summary_statistics.csv\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CHUNK SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFirst 10 chunks:\")\n",
        "print(chunk_stats_pd.head(10))\n",
        "print(f\"\\nLast 10 chunks:\")\n",
        "print(chunk_stats_pd.tail(10))\n",
        "\n",
        "# Aggregate statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE STATISTICS BY CHUNK TYPE\")\n",
        "print(\"=\"*80)\n",
        "summary_by_type = chunk_stats_pd.groupby('chunk_type').agg({\n",
        "    'unique_customers': ['sum', 'mean', 'std'],\n",
        "    'total_samples': ['sum', 'mean', 'std']\n",
        "})\n",
        "print(summary_by_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Critical Validation: Customer Uniqueness Across Chunks\n",
        "\n",
        "**MOST IMPORTANT CHECK**: Verify that each customer_id appears in exactly ONE chunk folder.\n",
        "\n",
        "This is the core isolation constraint - if violated, the modeling setup is broken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CRITICAL VALIDATION: CUSTOMER UNIQUENESS ACROSS CHUNKS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nChecking if any customer_id appears in multiple chunk folders...\")\n",
        "print(\"(This should return ZERO violations)\\n\")\n",
        "\n",
        "# Find customers that appear in multiple chunks\n",
        "customer_chunk_mapping = df_all.select(\"cust_id\", \"chunk\").distinct()\n",
        "\n",
        "customer_chunk_count = customer_chunk_mapping.groupBy(\"cust_id\").agg(\n",
        "    F.countDistinct(\"chunk\").alias(\"num_chunks\"),\n",
        "    F.collect_set(\"chunk\").alias(\"chunks\")\n",
        ")\n",
        "\n",
        "# Find violations (customers in multiple chunks)\n",
        "violations = customer_chunk_count.filter(F.col(\"num_chunks\") > 1)\n",
        "violation_count = violations.count()\n",
        "\n",
        "if violation_count == 0:\n",
        "    print(\"✓✓✓ VALIDATION PASSED ✓✓✓\")\n",
        "    print(\"   No customer appears in multiple chunks.\")\n",
        "    print(\"   Customer-level isolation is maintained correctly!\")\n",
        "else:\n",
        "    print(\"✗✗✗ VALIDATION FAILED ✗✗✗\")\n",
        "    print(f\"   Found {violation_count:,} customers appearing in multiple chunks!\")\n",
        "    print(\"   This violates the per-customer isolation constraint.\")\n",
        "    print(\"\\nFirst 20 violations:\")\n",
        "    violations_pd = violations.limit(20).toPandas()\n",
        "    print(violations_pd)\n",
        "    \n",
        "    # Save violations to file\n",
        "    violations_full = violations.toPandas()\n",
        "    save_pandas_to_csv_adls(violations_full, OUTPUT_PATH + \"CRITICAL_customer_chunk_violations.csv\")\n",
        "    print(f\"\\n   Full violation list saved\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CUSTOMER DISTRIBUTION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "total_unique_customers = customer_chunk_count.count()\n",
        "print(f\"Total unique customers across all chunks: {total_unique_customers:,}\")\n",
        "print(f\"Customers with violations (in multiple chunks): {violation_count:,}\")\n",
        "print(f\"Violation rate: {(violation_count/total_unique_customers*100):.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Split Confirmation: Train / Valid / OOT\n",
        "\n",
        "Verify the split configuration:\n",
        "- **Train**: Chunks 0-255, dates before 2024\n",
        "- **Valid**: Chunks 256-319, dates before 2024  \n",
        "- **OOT**: All chunks, dates in 2024+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SPLIT CONFIRMATION: TRAIN / VALID / OOT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Add split labels based on chunk and date\n",
        "df_with_split = df_all.withColumn(\n",
        "    \"split_label\",\n",
        "    F.when(\n",
        "        (F.col(\"efectv_dt\") >= OOT_START_DATE), \"OOT\"\n",
        "    ).when(\n",
        "        (F.col(\"chunk\") < 256) & (F.col(\"efectv_dt\") < OOT_START_DATE), \"train\"\n",
        "    ).when(\n",
        "        (F.col(\"chunk\") >= 256) & (F.col(\"efectv_dt\") < OOT_START_DATE), \"valid\"\n",
        "    ).otherwise(\"unknown\")\n",
        ")\n",
        "\n",
        "# Calculate split statistics\n",
        "split_stats = df_with_split.groupBy(\"split_label\").agg(\n",
        "    F.countDistinct(\"cust_id\").alias(\"unique_customers\"),\n",
        "    F.count(\"*\").alias(\"total_samples\"),\n",
        "    F.countDistinct(\"chunk\").alias(\"num_chunks\"),\n",
        "    F.min(\"efectv_dt\").alias(\"min_date\"),\n",
        "    F.max(\"efectv_dt\").alias(\"max_date\")\n",
        ").orderBy(\"split_label\")\n",
        "\n",
        "split_stats_pd = split_stats.toPandas()\n",
        "save_pandas_to_csv_adls(split_stats_pd, OUTPUT_PATH + \"split_confirmation_statistics.csv\")\n",
        "\n",
        "print(\"\\nSplit Statistics:\")\n",
        "print(split_stats_pd.to_string(index=False))\n",
        "\n",
        "# Check for unknown splits (should be zero)\n",
        "unknown_count = split_stats_pd[split_stats_pd['split_label'] == 'unknown']['total_samples'].sum()\n",
        "if unknown_count > 0:\n",
        "    print(f\"\\n✗ WARNING: Found {unknown_count:,} samples with 'unknown' split label!\")\n",
        "else:\n",
        "    print(\"\\n✓ All samples correctly assigned to train/valid/OOT splits\")\n",
        "\n",
        "# Calculate customer overlap between in-time and OOT\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CUSTOMER OVERLAP: IN-TIME vs OOT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "intime_customers = df_with_split.filter(\n",
        "    F.col(\"split_label\").isin([\"train\", \"valid\"])\n",
        ").select(\"cust_id\").distinct()\n",
        "\n",
        "oot_customers = df_with_split.filter(\n",
        "    F.col(\"split_label\") == \"OOT\"\n",
        ").select(\"cust_id\").distinct()\n",
        "\n",
        "# Count overlap\n",
        "overlap = intime_customers.join(oot_customers, on=\"cust_id\", how=\"inner\").count()\n",
        "total_intime = intime_customers.count()\n",
        "total_oot = oot_customers.count()\n",
        "\n",
        "print(f\"In-time unique customers: {total_intime:,}\")\n",
        "print(f\"OOT unique customers: {total_oot:,}\")\n",
        "print(f\"Customers appearing in both: {overlap:,}\")\n",
        "print(f\"Overlap percentage: {(overlap/total_intime*100):.2f}% of in-time customers\")\n",
        "print(\"\\nNote: Overlap is expected since same customers appear in both time periods.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Temporal Distribution by Split\n",
        "\n",
        "Analyze sample distribution over time for each split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Analyzing temporal distribution by split...\")\n",
        "\n",
        "# Count samples by month and split\n",
        "temporal_dist = df_with_split.groupBy(\"efectv_dt\", \"split_label\").agg(\n",
        "    F.countDistinct(\"cust_id\").alias(\"unique_customers\"),\n",
        "    F.count(\"*\").alias(\"total_samples\")\n",
        ").orderBy(\"efectv_dt\", \"split_label\")\n",
        "\n",
        "temporal_dist_pd = temporal_dist.toPandas().drop(\"unique_customers\", axis=1)\n",
        "save_pandas_to_csv_adls(temporal_dist_pd, OUTPUT_PATH + \"temporal_distribution_by_split.csv\")\n",
        "\n",
        "# Plot temporal distribution\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Plot 1: Total samples over time by split (no axes, just one plot)\n",
        "plt.figure(figsize=(16, 6))\n",
        "for split in ['train', 'valid', 'OOT']:\n",
        "    split_data = temporal_dist_pd[temporal_dist_pd['split_label'] == split]\n",
        "    if len(split_data) > 0:\n",
        "        plt.plot(split_data['efectv_dt'], split_data['total_samples'], \n",
        "                 marker='o', label=split, linewidth=2)\n",
        "\n",
        "plt.xlabel('Effective Date', fontsize=12)\n",
        "plt.ylabel('Total Samples', fontsize=12)\n",
        "plt.title('Sample Count Over Time by Split', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot_to_adls(fig, OUTPUT_PATH + \"temporal_distribution_plots.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary Report\n",
        "\n",
        "Generate final summary of all validation checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL VALIDATION SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Collect all key metrics\n",
        "summary_report = {\n",
        "    \"validation_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"sampling_ratio\": SAMPLING_RATIO,\n",
        "    \"total_chunks\": TOTAL_CHUNKS,\n",
        "    \"train_chunks\": len(TRAIN_CHUNKS),\n",
        "    \"valid_chunks\": len(VALID_CHUNKS),\n",
        "    \"total_samples\": int(df_all.count()),\n",
        "    \"total_unique_customers\": int(customer_chunk_count.count()),\n",
        "    \"customer_chunk_violations\": int(violation_count),\n",
        "    \"validation_passed\": violation_count == 0,\n",
        "}\n",
        "\n",
        "# Add split statistics\n",
        "for _, row in split_stats_pd.iterrows():\n",
        "    split = row['split_label']\n",
        "    summary_report[f\"{split}_unique_customers\"] = int(row['unique_customers'])\n",
        "    summary_report[f\"{split}_total_samples\"] = int(row['total_samples'])\n",
        "    summary_report[f\"{split}_num_chunks\"] = int(row['num_chunks'])\n",
        "\n",
        "# Save summary\n",
        "summary_df = pd.DataFrame([summary_report])\n",
        "save_pandas_to_csv_adls(summary_df, OUTPUT_PATH + \"validation_summary_report.csv\")\n",
        "\n",
        "# Print formatted summary\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"  Timestamp: {summary_report['validation_date']}\")\n",
        "print(f\"  Sampling Ratio: {summary_report['sampling_ratio']*100}%\")\n",
        "print(f\"\\nData Overview:\")\n",
        "print(f\"  Total Samples: {summary_report['total_samples']:,}\")\n",
        "print(f\"  Total Unique Customers: {summary_report['total_unique_customers']:,}\")\n",
        "print(f\"\\nChunk Configuration:\")\n",
        "print(f\"  Total Chunks: {summary_report['total_chunks']}\")\n",
        "print(f\"  Train Chunks: {summary_report['train_chunks']} (0-255)\")\n",
        "print(f\"  Valid Chunks: {summary_report['valid_chunks']} (256-319)\")\n",
        "print(f\"\\nCritical Validation:\")\n",
        "print(f\"  Customer Chunk Violations: {summary_report['customer_chunk_violations']:,}\")\n",
        "print(f\"  Validation Status: {'✓ PASSED' if summary_report['validation_passed'] else '✗ FAILED'}\")\n",
        "\n",
        "if 'train_unique_customers' in summary_report:\n",
        "    print(f\"\\nSplit Statistics:\")\n",
        "    print(f\"  Train - Customers: {summary_report.get('train_unique_customers', 0):,}, \"\n",
        "          f\"Samples: {summary_report.get('train_total_samples', 0):,}\")\n",
        "    print(f\"  Valid - Customers: {summary_report.get('valid_unique_customers', 0):,}, \"\n",
        "          f\"Samples: {summary_report.get('valid_total_samples', 0):,}\")\n",
        "    print(f\"  OOT   - Customers: {summary_report.get('OOT_unique_customers', 0):,}, \"\n",
        "          f\"Samples: {summary_report.get('OOT_total_samples', 0):,}\")\n",
        "\n",
        "print(f\"\\n✓ Summary report saved to {OUTPUT_PATH}validation_summary_report.csv\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATION COMPLETE\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup\n",
        "\n",
        "Unpersist cached dataframes to free up memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unpersist cached data\n",
        "df_all.unpersist()\n",
        "print(\"✓ Memory cleaned up\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
